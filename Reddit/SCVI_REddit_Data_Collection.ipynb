{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8nWKduBM0Y9",
        "outputId": "727c95fe-406c-4544-8ac8-1eb09bb00670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Function to convert datetime to Unix timestamp\n",
        "def to_timestamp(date_str):\n",
        "    dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "    return int(time.mktime(dt.timetuple()))\n",
        "\n",
        "# Prompt user for custom date ranges\n",
        "# input(\"Enter the start date (YYYY-MM-DD): \")\n",
        "start_date =\"2023-06-01\"\n",
        "end_date = \"2024-09-24\"\n",
        "\n",
        "start_timestamp = to_timestamp(start_date)\n",
        "end_timestamp = to_timestamp(end_date)\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search posts within the given date range\n",
        "    for post in subreddit.search(f'timestamp:{start_timestamp}..{end_timestamp}', syntax='cloudsearch', sort='new', limit=1000):\n",
        "        post_id = post.id\n",
        "        title = post.title\n",
        "        author = str(post.author) if post.author else 'Unknown'\n",
        "        score = post.score\n",
        "        post_url = post.url\n",
        "        text = post.selftext\n",
        "        media_filename = ''\n",
        "        has_media = 'No'\n",
        "        timestamp = post.created_utc\n",
        "        post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "        post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "        post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "        # Check if the post has an image or video\n",
        "        if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "            media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "            download_media(post.url, media_filename)\n",
        "            has_media = 'Yes'\n",
        "        elif post.url.endswith(('.mp4', '.webm')):\n",
        "            media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "            download_media(post.url, media_filename)\n",
        "            has_media = 'Yes'\n",
        "\n",
        "        # Write post data to CSV\n",
        "        writer.writerow({\n",
        "            'Post ID': post_id,\n",
        "            'Title': title,\n",
        "            'Author': author,\n",
        "            'Score': score,\n",
        "            'Post URL': post_url,\n",
        "            'Text': text,\n",
        "            'Media Filename': media_filename,\n",
        "            'Has Media': has_media,\n",
        "            'Demographics Region': 'Unknown',  # If demographic data is not available from Reddit, it can be handled this way\n",
        "            'Timestamp': timestamp,\n",
        "            'Date': post_date,\n",
        "            'Month': post_month,\n",
        "            'Year': post_year\n",
        "        })\n",
        "\n",
        "        print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGlhncGwNEjZ",
        "outputId": "adcbbb14-b1bf-4157-b91d-8bc4ebdf9249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Function to convert datetime to Unix timestamp\n",
        "def to_timestamp(date_str):\n",
        "    dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
        "    return int(time.mktime(dt.timetuple()))\n",
        "\n",
        "# Prompt user for custom date ranges\n",
        "start_date = input(\"Enter the start date (YYYY-MM-DD): \")\n",
        "end_date = input(\"Enter the end date (YYYY-MM-DD): \")\n",
        "\n",
        "start_timestamp = to_timestamp(start_date)\n",
        "end_timestamp = to_timestamp(end_date)\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Reddit's `subreddit.search()` method supports limited syntax like timestamps, but it requires appropriate filters\n",
        "    query = f'timestamp:{start_timestamp}..{end_timestamp}'\n",
        "\n",
        "    try:\n",
        "        # Ensure we are correctly formatting the search to work with Reddit's API\n",
        "        for post in subreddit.search(query, syntax='cloudsearch', sort='new', limit=1000):\n",
        "            post_id = post.id\n",
        "            title = post.title\n",
        "            author = str(post.author) if post.author else 'Unknown'\n",
        "            score = post.score\n",
        "            post_url = post.url\n",
        "            text = post.selftext\n",
        "            media_filename = ''\n",
        "            has_media = 'No'\n",
        "            timestamp = post.created_utc\n",
        "            post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "            post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "            post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "            # Check if the post has an image or video\n",
        "            if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "            elif post.url.endswith(('.mp4', '.webm')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "\n",
        "            # Write post data to CSV\n",
        "            writer.writerow({\n",
        "                'Post ID': post_id,\n",
        "                'Title': title,\n",
        "                'Author': author,\n",
        "                'Score': score,\n",
        "                'Post URL': post_url,\n",
        "                'Text': text,\n",
        "                'Media Filename': media_filename,\n",
        "                'Has Media': has_media,\n",
        "                'Demographics Region': 'Unknown',  # If demographic data is not available from Reddit, it can be handled this way\n",
        "                'Timestamp': timestamp,\n",
        "                'Date': post_date,\n",
        "                'Month': post_month,\n",
        "                'Year': post_year\n",
        "            })\n",
        "\n",
        "            print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieQlXHF1PweX",
        "outputId": "e2aa5bbe-44be-49d8-dc4c-68704532b2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the start date (YYYY-MM-DD): 2024-01-01\n",
            "Enter the end date (YYYY-MM-DD): 2024-06-30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        for post in subreddit.search(\" OR \".join(topics), syntax='lucene', sort='new', limit=1000):\n",
        "            post_id = post.id\n",
        "            title = post.title\n",
        "            author = str(post.author) if post.author else 'Unknown'\n",
        "            score = post.score\n",
        "            post_url = post.url\n",
        "            text = post.selftext\n",
        "            media_filename = ''\n",
        "            has_media = 'No'\n",
        "            timestamp = post.created_utc\n",
        "            post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "            post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "            post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "            # Determine the topic based on the title or content\n",
        "            if \"Is this a scam?\" in title or \"Is this a scam?\" in text:\n",
        "                topic = \"Is this a scam?\"\n",
        "            elif \"Victim of a scam\" in title or \"Victim of a scam\" in text:\n",
        "                topic = \"Victim of a scam\"\n",
        "            else:\n",
        "                topic = \"Unknown\"\n",
        "\n",
        "            # Check if the post has an image or video\n",
        "            if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "            elif post.url.endswith(('.mp4', '.webm')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "\n",
        "            # Write post data to CSV\n",
        "            writer.writerow({\n",
        "                'Post ID': post_id,\n",
        "                'Title': title,\n",
        "                'Author': author,\n",
        "                'Score': score,\n",
        "                'Post URL': post_url,\n",
        "                'Text': text,\n",
        "                'Media Filename': media_filename,\n",
        "                'Has Media': has_media,\n",
        "                'Topic': topic,\n",
        "                'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                'Timestamp': timestamp,\n",
        "                'Date': post_date,\n",
        "                'Month': post_month,\n",
        "                'Year': post_year\n",
        "            })\n",
        "\n",
        "            print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRD1E2RiQ7xi",
        "outputId": "3a313276-1fcc-4ba7-fd42-e00e355bdd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Processed post ID: 1fovcm2\n",
            "Downloaded: media/1fott3c_image.jpeg\n",
            "Processed post ID: 1fott3c\n",
            "Processed post ID: 1fot6kx\n",
            "Processed post ID: 1foseuh\n",
            "Processed post ID: 1forq8i\n",
            "Processed post ID: 1foqzrs\n",
            "Downloaded: media/1foqsmo_image.jpeg\n",
            "Processed post ID: 1foqsmo\n",
            "Processed post ID: 1foq9r2\n",
            "Processed post ID: 1fopppb\n",
            "Processed post ID: 1fop8e5\n",
            "Downloaded: media/1fongba_image.jpeg\n",
            "Processed post ID: 1fongba\n",
            "Processed post ID: 1fong93\n",
            "Processed post ID: 1fomvs0\n",
            "Processed post ID: 1fomgnq\n",
            "Processed post ID: 1folqaw\n",
            "Downloaded: media/1folnwn_image.jpeg\n",
            "Processed post ID: 1folnwn\n",
            "Processed post ID: 1fokets\n",
            "Processed post ID: 1fojaws\n",
            "Processed post ID: 1foh2y3\n",
            "Processed post ID: 1fogrrq\n",
            "Processed post ID: 1fogn81\n",
            "Processed post ID: 1fogfpf\n",
            "Processed post ID: 1fogehh\n",
            "Downloaded: media/1fog9fq_image.jpeg\n",
            "Processed post ID: 1fog9fq\n",
            "Processed post ID: 1fog639\n",
            "Processed post ID: 1fofqr7\n",
            "Downloaded: media/1fofjkf_image.jpeg\n",
            "Processed post ID: 1fofjkf\n",
            "Processed post ID: 1fofddq\n",
            "Processed post ID: 1fof1xu\n",
            "Processed post ID: 1foc5pt\n",
            "Downloaded: media/1fobc9e_image.jpeg\n",
            "Processed post ID: 1fobc9e\n",
            "Processed post ID: 1fo9j03\n",
            "Processed post ID: 1fo97so\n",
            "Processed post ID: 1fo92yz\n",
            "Processed post ID: 1fo6zgf\n",
            "Processed post ID: 1fo6u6y\n",
            "Processed post ID: 1fo5cvt\n",
            "Downloaded: media/1fo4sxe_image.jpeg\n",
            "Processed post ID: 1fo4sxe\n",
            "Processed post ID: 1fo4hop\n",
            "Downloaded: media/1fo49c2_image.jpeg\n",
            "Processed post ID: 1fo49c2\n",
            "Processed post ID: 1fo2kak\n",
            "Processed post ID: 1fo2ie2\n",
            "Processed post ID: 1fo2d3a\n",
            "Processed post ID: 1fo1var\n",
            "Processed post ID: 1fo0zjd\n",
            "Processed post ID: 1fo0qyu\n",
            "Processed post ID: 1fo0qwz\n",
            "Processed post ID: 1fo0g4x\n",
            "Processed post ID: 1fnzlb9\n",
            "Processed post ID: 1fnzk5b\n",
            "Processed post ID: 1fnzalf\n",
            "Processed post ID: 1fnyqqu\n",
            "Processed post ID: 1fnxy11\n",
            "Processed post ID: 1fnxuzg\n",
            "Downloaded: media/1fnxawu_image.jpeg\n",
            "Processed post ID: 1fnxawu\n",
            "Downloaded: media/1fnx1d5_image.jpeg\n",
            "Processed post ID: 1fnx1d5\n",
            "Processed post ID: 1fnw6ei\n",
            "Processed post ID: 1fnw11s\n",
            "Processed post ID: 1fnvid7\n",
            "Processed post ID: 1fnv9w3\n",
            "Processed post ID: 1fnun22\n",
            "Processed post ID: 1fntyvx\n",
            "Processed post ID: 1fntuqf\n",
            "Processed post ID: 1fnsubz\n",
            "Processed post ID: 1fnsgqq\n",
            "Processed post ID: 1fnscls\n",
            "Processed post ID: 1fnr5ur\n",
            "Processed post ID: 1fnqxsf\n",
            "Downloaded: media/1fnqtta_image.jpeg\n",
            "Processed post ID: 1fnqtta\n",
            "Processed post ID: 1fnqn0u\n",
            "Processed post ID: 1fnqbzn\n",
            "Processed post ID: 1fnpzqw\n",
            "Downloaded: media/1fnpv8r_image.jpeg\n",
            "Processed post ID: 1fnpv8r\n",
            "Processed post ID: 1fnpsfv\n",
            "Processed post ID: 1fnoq5o\n",
            "Processed post ID: 1fnody7\n",
            "Processed post ID: 1fnnl75\n",
            "Downloaded: media/1fnn5th_image.png\n",
            "Processed post ID: 1fnn5th\n",
            "Processed post ID: 1fnn4ib\n",
            "Processed post ID: 1fnmmr5\n",
            "Processed post ID: 1fnll3y\n",
            "Processed post ID: 1fnlc22\n",
            "Processed post ID: 1fnkth8\n",
            "Processed post ID: 1fnk4bo\n",
            "Processed post ID: 1fnjs2m\n",
            "Processed post ID: 1fnjo3h\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: media/1fnjeuz_image.jpeg\n",
            "Processed post ID: 1fnjeuz\n",
            "Processed post ID: 1fnhoqr\n",
            "Processed post ID: 1fngu0q\n",
            "Processed post ID: 1fngppi\n",
            "Processed post ID: 1fndmjr\n",
            "Processed post ID: 1fncn4f\n",
            "Processed post ID: 1fnc8wo\n",
            "Processed post ID: 1fnc7n2\n",
            "Processed post ID: 1fnaxc8\n",
            "Processed post ID: 1fnausr\n",
            "Processed post ID: 1fn9jp7\n",
            "Processed post ID: 1fn5m6d\n",
            "Processed post ID: 1fn55cq\n",
            "Processed post ID: 1fn4dcr\n",
            "Processed post ID: 1fn3z92\n",
            "Processed post ID: 1fn3k6h\n",
            "Downloaded: media/1fn1o4t_image.jpeg\n",
            "Processed post ID: 1fn1o4t\n",
            "Processed post ID: 1fn0aea\n",
            "Processed post ID: 1fmyef6\n",
            "Processed post ID: 1fmxkk2\n",
            "Processed post ID: 1fmwmrl\n",
            "Processed post ID: 1fmwlbl\n",
            "Processed post ID: 1fmvxta\n",
            "Processed post ID: 1fmvlqn\n",
            "Processed post ID: 1fmtn0f\n",
            "Processed post ID: 1fmt1l6\n",
            "Processed post ID: 1fmshms\n",
            "Processed post ID: 1fmmks3\n",
            "Processed post ID: 1fmksr6\n",
            "Processed post ID: 1fmkmqk\n",
            "Processed post ID: 1fmkfqa\n",
            "Processed post ID: 1fmik28\n",
            "Downloaded: media/1fmideq_image.png\n",
            "Processed post ID: 1fmideq\n",
            "Processed post ID: 1fmh53a\n",
            "Processed post ID: 1fmgowc\n",
            "Processed post ID: 1fmgbz8\n",
            "Processed post ID: 1fmfgf3\n",
            "Processed post ID: 1fmf0dp\n",
            "Processed post ID: 1fmed0y\n",
            "Processed post ID: 1fmde1z\n",
            "Processed post ID: 1fmdc9a\n",
            "Processed post ID: 1fmco21\n",
            "Processed post ID: 1fmcko8\n",
            "Processed post ID: 1fmaxer\n",
            "Processed post ID: 1fmafj6\n",
            "Downloaded: media/1fma2hb_image.jpeg\n",
            "Processed post ID: 1fma2hb\n",
            "Processed post ID: 1fma1wc\n",
            "Processed post ID: 1fm9swo\n",
            "Processed post ID: 1fm9807\n",
            "Processed post ID: 1fm97r3\n",
            "Processed post ID: 1fm89ix\n",
            "Processed post ID: 1fm7p7z\n",
            "Processed post ID: 1fm7lg3\n",
            "Processed post ID: 1fm6vxa\n",
            "Processed post ID: 1fm645a\n",
            "Processed post ID: 1fm4vva\n",
            "Processed post ID: 1fm2tno\n",
            "Processed post ID: 1fm1gkc\n",
            "Processed post ID: 1fm0wa2\n",
            "Processed post ID: 1fm0o1d\n",
            "Processed post ID: 1fm0gqd\n",
            "Processed post ID: 1flwxjy\n",
            "Processed post ID: 1fluaa5\n",
            "Processed post ID: 1flthg2\n",
            "Processed post ID: 1flruqm\n",
            "Processed post ID: 1flq9em\n",
            "Processed post ID: 1flnxu5\n",
            "Processed post ID: 1flmfum\n",
            "Processed post ID: 1fllnrl\n",
            "Processed post ID: 1flldu0\n",
            "Processed post ID: 1fll9mj\n",
            "Downloaded: media/1fll83a_image.jpeg\n",
            "Processed post ID: 1fll83a\n",
            "Processed post ID: 1flkqii\n",
            "Processed post ID: 1flkm1a\n",
            "Downloaded: media/1flhvtw_image.jpeg\n",
            "Processed post ID: 1flhvtw\n",
            "Processed post ID: 1flhm9o\n",
            "Processed post ID: 1flhld4\n",
            "Downloaded: media/1flgyu5_image.jpeg\n",
            "Processed post ID: 1flgyu5\n",
            "Processed post ID: 1flgleu\n",
            "Processed post ID: 1flg5dd\n",
            "Processed post ID: 1flfpiz\n",
            "Processed post ID: 1flf6ad\n",
            "Processed post ID: 1fle1t1\n",
            "Processed post ID: 1flc55o\n",
            "Processed post ID: 1flbwxd\n",
            "Downloaded: media/1flbrfn_image.jpeg\n",
            "Processed post ID: 1flbrfn\n",
            "Processed post ID: 1flbbb1\n",
            "Processed post ID: 1flb8ds\n",
            "Processed post ID: 1flay70\n",
            "Processed post ID: 1flaka0\n",
            "Processed post ID: 1fl9w4a\n",
            "Processed post ID: 1fl9pe4\n",
            "Processed post ID: 1fl9nhs\n",
            "Processed post ID: 1fl9jwc\n",
            "Processed post ID: 1fl7b3g\n",
            "Processed post ID: 1fl5fqp\n",
            "Processed post ID: 1fl4asi\n",
            "Processed post ID: 1fl42gi\n",
            "Processed post ID: 1fl3stw\n",
            "Processed post ID: 1fl3lh0\n",
            "Processed post ID: 1fl3f7e\n",
            "Processed post ID: 1fl34e8\n",
            "Processed post ID: 1fl323o\n",
            "Processed post ID: 1fl2dxk\n",
            "Processed post ID: 1fl0p2k\n",
            "Processed post ID: 1fl0ht0\n",
            "Processed post ID: 1fl0cya\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: media/1fl00di_image.jpeg\n",
            "Processed post ID: 1fl00di\n",
            "Processed post ID: 1fkz6p4\n",
            "Processed post ID: 1fkz31m\n",
            "Processed post ID: 1fkz2m8\n",
            "Processed post ID: 1fkyxp9\n",
            "Processed post ID: 1fkxm79\n",
            "Downloaded: media/1fkxd1w_image.jpeg\n",
            "Processed post ID: 1fkxd1w\n",
            "Processed post ID: 1fkuz3g\n",
            "Processed post ID: 1fkug5b\n",
            "Processed post ID: 1fkt5ao\n",
            "Processed post ID: 1fksj5v\n",
            "Processed post ID: 1fksghr\n",
            "Processed post ID: 1fkrocp\n",
            "Processed post ID: 1fkqjy5\n",
            "Processed post ID: 1fkqf4y\n",
            "Processed post ID: 1fkqdrl\n",
            "Processed post ID: 1fkq5of\n",
            "Downloaded: media/1fkplwb_image.jpeg\n",
            "Processed post ID: 1fkplwb\n",
            "Downloaded: media/1fkpeun_image.jpeg\n",
            "Processed post ID: 1fkpeun\n",
            "Processed post ID: 1fkp7nn\n",
            "Processed post ID: 1fkosih\n",
            "Processed post ID: 1fkokyo\n",
            "Processed post ID: 1fkok8o\n",
            "Processed post ID: 1fko3yw\n",
            "Processed post ID: 1fko33e\n",
            "Processed post ID: 1fkngdf\n",
            "Processed post ID: 1fkn57r\n",
            "Processed post ID: 1fkn4d8\n",
            "Downloaded: media/1fkmna7_image.jpeg\n",
            "Processed post ID: 1fkmna7\n",
            "Processed post ID: 1fkm8dn\n",
            "Processed post ID: 1fkltgr\n",
            "Processed post ID: 1fklt2h\n",
            "Processed post ID: 1fkl95j\n",
            "Processed post ID: 1fkkqgz\n",
            "Processed post ID: 1fkkgpx\n",
            "Processed post ID: 1fkkgo7\n",
            "Processed post ID: 1fkk3lk\n",
            "Processed post ID: 1fkj25b\n",
            "Processed post ID: 1fkiinw\n",
            "Processed post ID: 1fkhthm\n",
            "Processed post ID: 1fkhoaq\n",
            "Processed post ID: 1fkhkep\n",
            "Processed post ID: 1fkfu8c\n",
            "Processed post ID: 1fkes5p\n",
            "Processed post ID: 1fkcrp4\n",
            "Processed post ID: 1fkc34j\n",
            "Processed post ID: 1fkc25z\n",
            "Processed post ID: 1fkbsgk\n",
            "Processed post ID: 1fkalae\n",
            "Processed post ID: 1fkab0z\n",
            "Processed post ID: 1fk910j\n",
            "Processed post ID: 1fk7u4u\n",
            "Processed post ID: 1fk794j\n",
            "Processed post ID: 1fk67t5\n",
            "Processed post ID: 1fk5nsn\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# File to store previously fetched post IDs\n",
        "post_id_log = 'fetched_posts.txt'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Load previously fetched post IDs from the log file\n",
        "if os.path.exists(post_id_log):\n",
        "    with open(post_id_log, 'r') as f:\n",
        "        fetched_post_ids = set(f.strip() for f in f.readlines())\n",
        "else:\n",
        "    fetched_post_ids = set()\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        with open(post_id_log, 'a') as id_log_file:\n",
        "            for post in subreddit.search(\" OR \".join(topics), syntax='lucene', sort='new', limit=1000):\n",
        "                post_id = post.id\n",
        "\n",
        "                # Skip posts already fetched\n",
        "                if post_id in fetched_post_ids:\n",
        "                    print(f\"Skipping post {post_id} (already fetched)\")\n",
        "                    continue\n",
        "\n",
        "                title = post.title\n",
        "                author = str(post.author) if post.author else 'Unknown'\n",
        "                score = post.score\n",
        "                post_url = post.url\n",
        "                text = post.selftext\n",
        "                media_filename = ''\n",
        "                has_media = 'No'\n",
        "                timestamp = post.created_utc\n",
        "                post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                # Determine the topic based on the title or content\n",
        "                if \"Is this a scam?\" in title or \"Is this a scam?\" in text:\n",
        "                    topic = \"Is this a scam?\"\n",
        "                elif \"Victim of a scam\" in title or \"Victim of a scam\" in text:\n",
        "                    topic = \"Victim of a scam\"\n",
        "                else:\n",
        "                    topic = \"Unknown\"\n",
        "\n",
        "                # Check if the post has an image or video\n",
        "                if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                    media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                    download_media(post.url, media_filename)\n",
        "                    has_media = 'Yes'\n",
        "                elif post.url.endswith(('.mp4', '.webm')):\n",
        "                    media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                    download_media(post.url, media_filename)\n",
        "                    has_media = 'Yes'\n",
        "\n",
        "                # Write post data to CSV\n",
        "                writer.writerow({\n",
        "                    'Post ID': post_id,\n",
        "                    'Title': title,\n",
        "                    'Author': author,\n",
        "                    'Score': score,\n",
        "                    'Post URL': post_url,\n",
        "                    'Text': text,\n",
        "                    'Media Filename': media_filename,\n",
        "                    'Has Media': has_media,\n",
        "                    'Topic': topic,\n",
        "                    'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                    'Timestamp': timestamp,\n",
        "                    'Date': post_date,\n",
        "                    'Month': post_month,\n",
        "                    'Year': post_year\n",
        "                })\n",
        "\n",
        "                print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "                # Save the post ID to the log file\n",
        "                id_log_file.write(f\"{post_id}\\n\")\n",
        "                fetched_post_ids.add(post_id)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "id": "Kqro5QdXSXtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PT94vQKYeQDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        for post in subreddit.search(\" OR \".join(topics), syntax='lucene', sort='new', limit=1000):\n",
        "            post_id = post.id\n",
        "            title = post.title\n",
        "            author = str(post.author) if post.author else 'Unknown'\n",
        "            score = post.score\n",
        "            post_url = post.url\n",
        "            text = post.selftext\n",
        "            media_filename = ''\n",
        "            has_media = 'No'\n",
        "            timestamp = post.created_utc\n",
        "            post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "            post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "            post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "            # Determine the topic based on the title or content\n",
        "            if \"Is this a scam?\" in title or \"Is this a scam?\" in text:\n",
        "                topic = \"Is this a scam?\"\n",
        "            elif \"Victim of a scam\" in title or \"Victim of a scam\" in text:\n",
        "                topic = \"Victim of a scam\"\n",
        "            else:\n",
        "                topic = \"Unknown\"\n",
        "\n",
        "            # Check if the post has an image or video\n",
        "            if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "            elif post.url.endswith(('.mp4', '.webm')):\n",
        "                media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                download_media(post.url, media_filename)\n",
        "                has_media = 'Yes'\n",
        "\n",
        "            # Only process posts with media files and relevant topics\n",
        "            if has_media == 'Yes' and topic in topics:\n",
        "                # Write post data to CSV\n",
        "                writer.writerow({\n",
        "                    'Post ID': post_id,\n",
        "                    'Title': title,\n",
        "                    'Author': author,\n",
        "                    'Score': score,\n",
        "                    'Post URL': post_url,\n",
        "                    'Text': text,\n",
        "                    'Media Filename': media_filename,\n",
        "                    'Has Media': has_media,\n",
        "                    'Topic': topic,\n",
        "                    'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                    'Timestamp': timestamp,\n",
        "                    'Date': post_date,\n",
        "                    'Month': post_month,\n",
        "                    'Year': post_year\n",
        "                })\n",
        "\n",
        "                print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIR8BNZiT7-y",
        "outputId": "3099ce29-fa3b-4fb2-8e0c-b22fb6c4dd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Downloaded: media/1fott3c_image.jpeg\n",
            "Downloaded: media/1foqsmo_image.jpeg\n",
            "Downloaded: media/1fongba_image.jpeg\n",
            "Downloaded: media/1folnwn_image.jpeg\n",
            "Downloaded: media/1fog9fq_image.jpeg\n",
            "Downloaded: media/1fofjkf_image.jpeg\n",
            "Downloaded: media/1fobc9e_image.jpeg\n",
            "Downloaded: media/1fo4sxe_image.jpeg\n",
            "Downloaded: media/1fo49c2_image.jpeg\n",
            "Downloaded: media/1fnxawu_image.jpeg\n",
            "Downloaded: media/1fnx1d5_image.jpeg\n",
            "Downloaded: media/1fnqtta_image.jpeg\n",
            "Processed post ID: 1fnqtta\n",
            "Downloaded: media/1fnpv8r_image.jpeg\n",
            "Downloaded: media/1fnn5th_image.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: media/1fnjeuz_image.jpeg\n",
            "Downloaded: media/1fn1o4t_image.jpeg\n",
            "Downloaded: media/1fmideq_image.png\n",
            "Downloaded: media/1fma2hb_image.jpeg\n",
            "Downloaded: media/1fll83a_image.jpeg\n",
            "Downloaded: media/1flhvtw_image.jpeg\n",
            "Downloaded: media/1flgyu5_image.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: media/1flbrfn_image.jpeg\n",
            "Downloaded: media/1fl00di_image.jpeg\n",
            "Downloaded: media/1fkxd1w_image.jpeg\n",
            "Downloaded: media/1fkplwb_image.jpeg\n",
            "Downloaded: media/1fkpeun_image.jpeg\n",
            "Downloaded: media/1fkmna7_image.jpeg\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Function to check if the post has media\n",
        "def has_media(post):\n",
        "    return post.url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.mp4', '.webm'))\n",
        "\n",
        "# Open CSV file for writing\n",
        "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    post_count = 0\n",
        "    target_post_count = 1000\n",
        "\n",
        "    try:\n",
        "        for topic in topics:\n",
        "            # Search posts with the topic in the title or text\n",
        "            for post in subreddit.search(topic, sort='new', limit=500):  # Adjust the limit per query\n",
        "                if post_count >= target_post_count:\n",
        "                    break\n",
        "\n",
        "                post_id = post.id\n",
        "                title = post.title\n",
        "                author = str(post.author) if post.author else 'Unknown'\n",
        "                score = post.score\n",
        "                post_url = post.url\n",
        "                text = post.selftext\n",
        "                media_filename = ''\n",
        "                has_media_flag = 'No'\n",
        "                timestamp = post.created_utc\n",
        "                post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                # Check if the post has media\n",
        "                if has_media(post):\n",
        "                    if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "                    elif post.url.endswith(('.mp4', '.webm')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "\n",
        "                    has_media_flag = 'Yes'\n",
        "\n",
        "                # Write post data to CSV, regardless of whether there is media\n",
        "                writer.writerow({\n",
        "                    'Post ID': post_id,\n",
        "                    'Title': title,\n",
        "                    'Author': author,\n",
        "                    'Score': score,\n",
        "                    'Post URL': post_url,\n",
        "                    'Text': text,\n",
        "                    'Media Filename': media_filename,\n",
        "                    'Has Media': has_media_flag,\n",
        "                    'Topic': topic,\n",
        "                    'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                    'Timestamp': timestamp,\n",
        "                    'Date': post_date,\n",
        "                    'Month': post_month,\n",
        "                    'Year': post_year\n",
        "                })\n",
        "\n",
        "                post_count += 1\n",
        "                print(f\"Processed post ID: {post_id}, Total posts processed: {post_count}\")\n",
        "\n",
        "                # Sleep between requests to avoid hitting rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "        if post_count == 0:\n",
        "            print(\"No posts found for the specified topics.\")\n",
        "        elif post_count < target_post_count:\n",
        "            print(f\"Only {post_count} posts found. Target was {target_post_count}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "SZuC2fT3VySI",
        "outputId": "1fe4cb42-ac78-4fe3-9f03-0a08a87cd8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Downloaded: media/1fow3eu_image.jpeg\n",
            "Processed post ID: 1fow3eu, Total posts processed: 1\n",
            "Processed post ID: 1fovnon, Total posts processed: 2\n",
            "Processed post ID: 1fovcm2, Total posts processed: 3\n",
            "Processed post ID: 1fou993, Total posts processed: 4\n",
            "Downloaded: media/1fott3c_image.jpeg\n",
            "Processed post ID: 1fott3c, Total posts processed: 5\n",
            "Downloaded: media/1fotohk_image.png\n",
            "Processed post ID: 1fotohk, Total posts processed: 6\n",
            "Processed post ID: 1fot6kx, Total posts processed: 7\n",
            "Processed post ID: 1foseuh, Total posts processed: 8\n",
            "Downloaded: media/1fosdal_image.jpeg\n",
            "Processed post ID: 1fosdal, Total posts processed: 9\n",
            "Processed post ID: 1forq8i, Total posts processed: 10\n",
            "Processed post ID: 1foqzrs, Total posts processed: 11\n",
            "Downloaded: media/1foqpwr_image.jpeg\n",
            "Processed post ID: 1foqpwr, Total posts processed: 12\n",
            "Processed post ID: 1foq9r2, Total posts processed: 13\n",
            "Processed post ID: 1fopppb, Total posts processed: 14\n",
            "Processed post ID: 1fopdi4, Total posts processed: 15\n",
            "Processed post ID: 1fop8e5, Total posts processed: 16\n",
            "Processed post ID: 1fop1fk, Total posts processed: 17\n",
            "Downloaded: media/1fongba_image.jpeg\n",
            "Processed post ID: 1fongba, Total posts processed: 18\n",
            "Processed post ID: 1fong93, Total posts processed: 19\n",
            "Processed post ID: 1fomvs0, Total posts processed: 20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5aee8fe07fc7>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0mmedia_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{post_id}_image{os.path.splitext(post.url)[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                         \u001b[0mdownload_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.webm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0mmedia_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'media'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{post_id}_video{os.path.splitext(post.url)[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5aee8fe07fc7>\u001b[0m in \u001b[0;36mdownload_media\u001b[0;34m(url, filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##keeping log of post ID and skipping:"
      ],
      "metadata": {
        "id": "z8b-h1snuJ8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# File to store previously fetched post IDs\n",
        "post_id_log = 'fetched_posts.txt'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Load previously fetched post IDs from the log file\n",
        "if os.path.exists(post_id_log):\n",
        "    with open(post_id_log, 'r') as f:\n",
        "        fetched_post_ids = set(f.strip() for f in f.readlines())\n",
        "else:\n",
        "    fetched_post_ids = set()\n",
        "\n",
        "# Open CSV file for appending (to avoid overwriting previous data)\n",
        "with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # If the CSV file is new, write the header row\n",
        "    if os.stat(csv_file).st_size == 0:\n",
        "        writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        with open(post_id_log, 'a') as id_log_file:\n",
        "            # Use 'OR' logic to search for the two topics\n",
        "            for topic in topics:\n",
        "                print(f\"Searching for topic: {topic}\")\n",
        "                for post in subreddit.search(topic, sort='relevance', limit=1000):  # Adjust limit as needed\n",
        "                    post_id = post.id\n",
        "\n",
        "                    # Skip posts already fetched\n",
        "                    if post_id in fetched_post_ids:\n",
        "                        print(f\"Skipping post {post_id} (already fetched)\")\n",
        "                        continue\n",
        "\n",
        "                    title = post.title\n",
        "                    author = str(post.author) if post.author else 'Unknown'\n",
        "                    score = post.score\n",
        "                    post_url = post.url\n",
        "                    text = post.selftext\n",
        "                    media_filename = ''\n",
        "                    has_media = 'No'\n",
        "                    timestamp = post.created_utc\n",
        "                    post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                    post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                    post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                    # Check if the post has an image or video\n",
        "                    if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "                    elif post.url.endswith(('.mp4', '.webm')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "\n",
        "                    # Write post data to CSV\n",
        "                    writer.writerow({\n",
        "                        'Post ID': post_id,\n",
        "                        'Title': title,\n",
        "                        'Author': author,\n",
        "                        'Score': score,\n",
        "                        'Post URL': post_url,\n",
        "                        'Text': text,\n",
        "                        'Media Filename': media_filename,\n",
        "                        'Has Media': has_media,\n",
        "                        'Topic': topic,\n",
        "                        'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                        'Timestamp': timestamp,\n",
        "                        'Date': post_date,\n",
        "                        'Month': post_month,\n",
        "                        'Year': post_year\n",
        "                    })\n",
        "\n",
        "                    print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "                    # Save the post ID to the log file\n",
        "                    id_log_file.write(f\"{post_id}\\n\")\n",
        "                    fetched_post_ids.add(post_id)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFFlNCOoeRpi",
        "outputId": "ce2557ed-dd31-4480-837d-c8d275a3ef66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Searching for topic: Is this a scam?\n",
            "Skipping post 1eka551 (already fetched)\n",
            "Skipping post 17etdax (already fetched)\n",
            "Processed post ID: 1fvc8cu\n",
            "Skipping post 1fg3e0v (already fetched)\n",
            "Skipping post 1evgmu6 (already fetched)\n",
            "Skipping post 1ef1753 (already fetched)\n",
            "Skipping post 18m8szv (already fetched)\n",
            "Skipping post 1adqegr (already fetched)\n",
            "Skipping post 18x0m8o (already fetched)\n",
            "Skipping post 18rrg2v (already fetched)\n",
            "Skipping post tignil (already fetched)\n",
            "Skipping post 13g2u3t (already fetched)\n",
            "Skipping post 1f1hqam (already fetched)\n",
            "Skipping post 144tr42 (already fetched)\n",
            "Skipping post 12shfkk (already fetched)\n",
            "Skipping post 162eatx (already fetched)\n",
            "Skipping post 1ernrzw (already fetched)\n",
            "Skipping post 1340nzg (already fetched)\n",
            "Skipping post wxf964 (already fetched)\n",
            "Skipping post 17bsrkb (already fetched)\n",
            "Downloaded: media/1ftogf6_image.jpeg\n",
            "Processed post ID: 1ftogf6\n",
            "Skipping post 13mjrm7 (already fetched)\n",
            "Skipping post 184278p (already fetched)\n",
            "Skipping post xhr8v1 (already fetched)\n",
            "Skipping post 15uzh01 (already fetched)\n",
            "Skipping post 1fc79mz (already fetched)\n",
            "Skipping post 16gaubi (already fetched)\n",
            "Skipping post 172p8ya (already fetched)\n",
            "Skipping post 1fmfg3s (already fetched)\n",
            "Skipping post 109b7yl (already fetched)\n",
            "Processed post ID: 1fruti8\n",
            "Skipping post 1aeerbn (already fetched)\n",
            "Skipping post 169u5um (already fetched)\n",
            "Skipping post 13gybe9 (already fetched)\n",
            "Skipping post 1edfslk (already fetched)\n",
            "Skipping post 1dum0pu (already fetched)\n",
            "Skipping post 14xljtu (already fetched)\n",
            "Skipping post 15xx04e (already fetched)\n",
            "Skipping post 1ec0q4f (already fetched)\n",
            "Skipping post 16puh7g (already fetched)\n",
            "Skipping post twac9p (already fetched)\n",
            "Skipping post 118fwrf (already fetched)\n",
            "Skipping post 143nmlf (already fetched)\n",
            "Skipping post xeiggs (already fetched)\n",
            "Skipping post 1dyecih (already fetched)\n",
            "Skipping post skzvej (already fetched)\n",
            "Skipping post wchm3n (already fetched)\n",
            "Skipping post 1ala1ox (already fetched)\n",
            "Skipping post 1bncgx9 (already fetched)\n",
            "Skipping post xa06mu (already fetched)\n",
            "Skipping post 1eholi7 (already fetched)\n",
            "Skipping post 1cas3a1 (already fetched)\n",
            "Skipping post 16vgjwa (already fetched)\n",
            "Skipping post 1enjoxl (already fetched)\n",
            "Skipping post 17v7g9v (already fetched)\n",
            "Skipping post 1cf457w (already fetched)\n",
            "Skipping post 136vi6a (already fetched)\n",
            "Skipping post 10dfqkw (already fetched)\n",
            "Skipping post 1bmyndc (already fetched)\n",
            "Skipping post wjttrp (already fetched)\n",
            "Skipping post 18gnq6y (already fetched)\n",
            "Skipping post vlvosg (already fetched)\n",
            "Skipping post 12con7k (already fetched)\n",
            "Skipping post 18yex5w (already fetched)\n",
            "Skipping post 1el0a2q (already fetched)\n",
            "Skipping post 1afz9hd (already fetched)\n",
            "Skipping post 1dey91r (already fetched)\n",
            "Skipping post 179drc4 (already fetched)\n",
            "Skipping post 111npae (already fetched)\n",
            "Skipping post 1eqv2is (already fetched)\n",
            "Skipping post 15830in (already fetched)\n",
            "Skipping post 1849cth (already fetched)\n",
            "Skipping post 1983x07 (already fetched)\n",
            "Skipping post ww18u2 (already fetched)\n",
            "Skipping post 11ek4hj (already fetched)\n",
            "Skipping post wua0mh (already fetched)\n",
            "Skipping post 1ezfv85 (already fetched)\n",
            "Skipping post 16j05e5 (already fetched)\n",
            "Skipping post 1c0s6ad (already fetched)\n",
            "Skipping post 1f0gecp (already fetched)\n",
            "Skipping post 16k7h8o (already fetched)\n",
            "Skipping post 122up7j (already fetched)\n",
            "Skipping post 16litg2 (already fetched)\n",
            "Skipping post 10ym89r (already fetched)\n",
            "Skipping post 17onnyd (already fetched)\n",
            "Skipping post thy69v (already fetched)\n",
            "Skipping post 1dowi4p (already fetched)\n",
            "Skipping post uf4qi6 (already fetched)\n",
            "Skipping post ue15uj (already fetched)\n",
            "Skipping post 1f3vwh0 (already fetched)\n",
            "Skipping post 1cg5nf7 (already fetched)\n",
            "Skipping post zu35yc (already fetched)\n",
            "Skipping post v9xwvu (already fetched)\n",
            "Skipping post 1d17ief (already fetched)\n",
            "Skipping post 17b52nm (already fetched)\n",
            "Skipping post 1eunopq (already fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: media/1fun26b_image.jpeg\n",
            "Processed post ID: 1fun26b\n",
            "Skipping post 15cxvyn (already fetched)\n",
            "Skipping post 1f7go5q (already fetched)\n",
            "Skipping post 1afijwc (already fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 11bahld (already fetched)\n",
            "Skipping post 160vnkr (already fetched)\n",
            "Skipping post vetwf0 (already fetched)\n",
            "Skipping post 193m0wu (already fetched)\n",
            "Skipping post 1b4vbgj (already fetched)\n",
            "Skipping post tpc7fs (already fetched)\n",
            "Skipping post z338lw (already fetched)\n",
            "Skipping post t6mx4y (already fetched)\n",
            "Skipping post sohkov (already fetched)\n",
            "Skipping post 1chth31 (already fetched)\n",
            "Skipping post wfo0iy (already fetched)\n",
            "Skipping post 170wnph (already fetched)\n",
            "Skipping post 1ap8e36 (already fetched)\n",
            "Skipping post 177z6d5 (already fetched)\n",
            "Skipping post 18y2h9m (already fetched)\n",
            "Skipping post 1fj9enz (already fetched)\n",
            "Skipping post 17p0v7r (already fetched)\n",
            "Skipping post 1apsxjd (already fetched)\n",
            "Skipping post 10jd38z (already fetched)\n",
            "Skipping post 1ej7c8l (already fetched)\n",
            "Skipping post zea93h (already fetched)\n",
            "Skipping post 11xi5h7 (already fetched)\n",
            "Skipping post 18x5dq0 (already fetched)\n",
            "Skipping post 11sehi8 (already fetched)\n",
            "Skipping post swc1no (already fetched)\n",
            "Skipping post 15b99p4 (already fetched)\n",
            "Skipping post 18x0rmz (already fetched)\n",
            "Skipping post th1u5u (already fetched)\n",
            "Skipping post x7h9yb (already fetched)\n",
            "Skipping post 1ckasf4 (already fetched)\n",
            "Skipping post x03zd4 (already fetched)\n",
            "Skipping post 1ey3scl (already fetched)\n",
            "Skipping post xtao1q (already fetched)\n",
            "Skipping post 1blytmp (already fetched)\n",
            "Skipping post 10zbb6r (already fetched)\n",
            "Skipping post yli3kl (already fetched)\n",
            "Skipping post sqx5fv (already fetched)\n",
            "Skipping post 1e0e5dy (already fetched)\n",
            "Skipping post fxs7ul (already fetched)\n",
            "Skipping post 11y417f (already fetched)\n",
            "Skipping post 1cmn9vw (already fetched)\n",
            "Skipping post 1c4tya0 (already fetched)\n",
            "Skipping post mg7orc (already fetched)\n",
            "Skipping post 1epd879 (already fetched)\n",
            "Skipping post 187n7up (already fetched)\n",
            "Skipping post 1f0u3eq (already fetched)\n",
            "Skipping post ultjz2 (already fetched)\n",
            "Skipping post 18ziom2 (already fetched)\n",
            "Skipping post 1b8ttd0 (already fetched)\n",
            "Skipping post uvuz8e (already fetched)\n",
            "Skipping post p1pk6u (already fetched)\n",
            "Skipping post 1fkyq1g (already fetched)\n",
            "Skipping post umys2d (already fetched)\n",
            "Skipping post uao9v3 (already fetched)\n",
            "Skipping post tp7y3e (already fetched)\n",
            "Skipping post 17mw7ze (already fetched)\n",
            "Skipping post 1dreq80 (already fetched)\n",
            "Skipping post ygq7vy (already fetched)\n",
            "Skipping post 1ar6bpr (already fetched)\n",
            "Skipping post tjgs3a (already fetched)\n",
            "Skipping post 1eujncx (already fetched)\n",
            "Skipping post ig3841 (already fetched)\n",
            "Skipping post 195ej3q (already fetched)\n",
            "Skipping post 10w59vs (already fetched)\n",
            "Skipping post wxmpug (already fetched)\n",
            "Skipping post 1c20hq9 (already fetched)\n",
            "Skipping post 18oq1l5 (already fetched)\n",
            "Skipping post 15ltsip (already fetched)\n",
            "Skipping post x8gh0a (already fetched)\n",
            "Skipping post zx00tn (already fetched)\n",
            "Skipping post zdi987 (already fetched)\n",
            "Skipping post 10at4e6 (already fetched)\n",
            "Skipping post osfuwe (already fetched)\n",
            "Skipping post 17l4yzh (already fetched)\n",
            "Skipping post 128qosq (already fetched)\n",
            "Skipping post 1daoqww (already fetched)\n",
            "Skipping post pcmnxa (already fetched)\n",
            "Skipping post 15w40g6 (already fetched)\n",
            "Skipping post xe3fra (already fetched)\n",
            "Skipping post 1f3dyol (already fetched)\n",
            "Skipping post 1dl5kpy (already fetched)\n",
            "Skipping post 1c5ks0k (already fetched)\n",
            "Skipping post 1awvnsd (already fetched)\n",
            "Skipping post 1fi8e4m (already fetched)\n",
            "Skipping post 18tmkll (already fetched)\n",
            "Skipping post 1bpxnkz (already fetched)\n",
            "Skipping post 1f2xgm8 (already fetched)\n",
            "Skipping post 18f1d6w (already fetched)\n",
            "Skipping post 1fgr4u2 (already fetched)\n",
            "Skipping post 199w0xw (already fetched)\n",
            "Skipping post vw1mp0 (already fetched)\n",
            "Skipping post 19cupcd (already fetched)\n",
            "Skipping post 1becsk5 (already fetched)\n",
            "Skipping post 1am7vog (already fetched)\n",
            "Skipping post mrt10k (already fetched)\n",
            "Skipping post 1edvism (already fetched)\n",
            "Skipping post 12gnzid (already fetched)\n",
            "Skipping post 16w82bf (already fetched)\n",
            "Skipping post twzqgh (already fetched)\n",
            "Skipping post wdoty7 (already fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 164f3bs (already fetched)\n",
            "Skipping post 1d7ez86 (already fetched)\n",
            "Skipping post 17isxio (already fetched)\n",
            "Skipping post z9y9zs (already fetched)\n",
            "Skipping post 16a608n (already fetched)\n",
            "Skipping post 1dk6tit (already fetched)\n",
            "Skipping post xrrnnr (already fetched)\n",
            "Skipping post 10chpvd (already fetched)\n",
            "Processed post ID: 1fqcw4k\n",
            "Skipping post 1cet2eb (already fetched)\n",
            "Skipping post zvpb1l (already fetched)\n",
            "Skipping post 10epo12 (already fetched)\n",
            "Skipping post uem9wy (already fetched)\n",
            "Skipping post 1czydpc (already fetched)\n",
            "Skipping post 1c2aepj (already fetched)\n",
            "Skipping post 17tnaal (already fetched)\n",
            "Skipping post sa3hbp (already fetched)\n",
            "Skipping post 1dqr825 (already fetched)\n",
            "Skipping post 1c3uytf (already fetched)\n",
            "Skipping post qfe69f (already fetched)\n",
            "Processed post ID: 1febyxg\n",
            "Skipping post z3jaig (already fetched)\n",
            "Skipping post zxlw3v (already fetched)\n",
            "Skipping post txmjxl (already fetched)\n",
            "Skipping post vto8t3 (already fetched)\n",
            "Skipping post tm8506 (already fetched)\n",
            "Skipping post 18k8ika (already fetched)\n",
            "Skipping post 18w0uiv (already fetched)\n",
            "Skipping post pij1bp (already fetched)\n",
            "Skipping post 10oa2c5 (already fetched)\n",
            "Skipping post 1bklk7w (already fetched)\n",
            "Skipping post 1f5o4es (already fetched)\n",
            "Skipping post 1ffwfsd (already fetched)\n",
            "Skipping post 1evg8pd (already fetched)\n",
            "Skipping post 1f43dzz (already fetched)\n",
            "Skipping post z1tms3 (already fetched)\n",
            "Skipping post 13oz73j (already fetched)\n",
            "Skipping post 15gjqqt (already fetched)\n",
            "Skipping post 16fgrpv (already fetched)\n",
            "Searching for topic: Victim of a scam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed post ID: 1fxnf3d\n",
            "Skipping post 1ckv96a (already fetched)\n",
            "Skipping post z6jnki (already fetched)\n",
            "Skipping post 12sbhgq (already fetched)\n",
            "Skipping post 10ym89r (already fetched)\n",
            "Skipping post 1dhhe76 (already fetched)\n",
            "Skipping post 167g4d1 (already fetched)\n",
            "Skipping post 18stkax (already fetched)\n",
            "Skipping post mg7orc (already fetched)\n",
            "Skipping post 1bgnrzz (already fetched)\n",
            "Skipping post 1ecpgb1 (already fetched)\n",
            "Processed post ID: 1fvnmc3\n",
            "Skipping post 18mg88c (already fetched)\n",
            "Processed post ID: 1fq8zgk\n",
            "Skipping post 15qgzam (already fetched)\n",
            "Skipping post 1b91hmr (already fetched)\n",
            "Processed post ID: 1fur0qp\n",
            "Skipping post 1eec5u2 (already fetched)\n",
            "Skipping post 1fbzs2x (already fetched)\n",
            "Skipping post 1fixvx2 (already fetched)\n",
            "Skipping post 1c09bct (already fetched)\n",
            "Skipping post mz5amg (already fetched)\n",
            "Skipping post 1df816u (already fetched)\n",
            "Skipping post ussjk4 (already fetched)\n",
            "Skipping post 1ekwok0 (already fetched)\n",
            "Skipping post 1e4eete (already fetched)\n",
            "Skipping post 1edscan (already fetched)\n",
            "Skipping post vgqlm1 (already fetched)\n",
            "Skipping post 1bngusl (already fetched)\n",
            "Skipping post 1dml0hc (already fetched)\n",
            "Skipping post 1bh00cu (already fetched)\n",
            "Skipping post vja8eu (already fetched)\n",
            "Skipping post 1e14irq (already fetched)\n",
            "Skipping post 1dg00u0 (already fetched)\n",
            "Skipping post 13telne (already fetched)\n",
            "Skipping post bzbgr2 (already fetched)\n",
            "Skipping post 1cna3z7 (already fetched)\n",
            "Skipping post 11qu406 (already fetched)\n",
            "Skipping post 1c2gy5u (already fetched)\n",
            "Skipping post n03v20 (already fetched)\n",
            "Skipping post 1ch0fa1 (already fetched)\n",
            "Skipping post skyd0a (already fetched)\n",
            "Skipping post 1bxl65i (already fetched)\n",
            "Skipping post 1afj7iv (already fetched)\n",
            "Skipping post 1cdksj2 (already fetched)\n",
            "Skipping post 1csvrlt (already fetched)\n",
            "Skipping post 1dsmg6h (already fetched)\n",
            "Skipping post 1ae75mx (already fetched)\n",
            "Skipping post 1625bcu (already fetched)\n",
            "Skipping post wh3tu7 (already fetched)\n",
            "Skipping post 11t6vyt (already fetched)\n",
            "Skipping post 1c06bnd (already fetched)\n",
            "Skipping post 16dnhkh (already fetched)\n",
            "Skipping post mbonjl (already fetched)\n",
            "Skipping post n00o17 (already fetched)\n",
            "Skipping post 1axcqmf (already fetched)\n",
            "Skipping post 1aeimhr (already fetched)\n",
            "Skipping post uut6uq (already fetched)\n",
            "Skipping post 15dh7nc (already fetched)\n",
            "Skipping post 18vnc36 (already fetched)\n",
            "Skipping post r38j4d (already fetched)\n",
            "Skipping post 11513go (already fetched)\n",
            "Skipping post 19b8n8w (already fetched)\n",
            "Skipping post 179k9qy (already fetched)\n",
            "Skipping post na8oax (already fetched)\n",
            "Skipping post 193j0b9 (already fetched)\n",
            "Skipping post 1910nqo (already fetched)\n",
            "Skipping post 1e5id8u (already fetched)\n",
            "Skipping post u203uc (already fetched)\n",
            "Skipping post 16ut0fx (already fetched)\n",
            "Skipping post 1bokeaj (already fetched)\n",
            "Skipping post 15twtbk (already fetched)\n",
            "Skipping post 1fo6zgf (already fetched)\n",
            "Skipping post 15835wc (already fetched)\n",
            "Skipping post u9qvre (already fetched)\n",
            "Skipping post 15wx403 (already fetched)\n",
            "Skipping post 13d6mq3 (already fetched)\n",
            "Skipping post 13o6jbr (already fetched)\n",
            "Skipping post 1919230 (already fetched)\n",
            "Skipping post 16dpgtt (already fetched)\n",
            "Skipping post 11lenb0 (already fetched)\n",
            "Skipping post 17v7g9v (already fetched)\n",
            "Skipping post vlzudn (already fetched)\n",
            "Skipping post 1dzyzs5 (already fetched)\n",
            "Skipping post 13jrkiw (already fetched)\n",
            "Skipping post ns5o7b (already fetched)\n",
            "Skipping post 1fm9swo (already fetched)\n",
            "Skipping post 196gqch (already fetched)\n",
            "Skipping post ltgr6b (already fetched)\n",
            "Skipping post vynyco (already fetched)\n",
            "Processed post ID: 1fs3mcd\n",
            "Skipping post udbjrh (already fetched)\n",
            "Skipping post 17n93ei (already fetched)\n",
            "Skipping post vq6hil (already fetched)\n",
            "Skipping post y87d8b (already fetched)\n",
            "Skipping post 18im5yn (already fetched)\n",
            "Skipping post p8vdia (already fetched)\n",
            "Skipping post uxxayx (already fetched)\n",
            "Skipping post xo6w8t (already fetched)\n",
            "Skipping post v48eq2 (already fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post ws2fxj (already fetched)\n",
            "Skipping post 1c11azw (already fetched)\n",
            "Skipping post 1650zv7 (already fetched)\n",
            "Skipping post 1aoeol0 (already fetched)\n",
            "Skipping post ryp4fg (already fetched)\n",
            "Skipping post svrbi3 (already fetched)\n",
            "Processed post ID: 1fqrejs\n",
            "Skipping post yrwh4z (already fetched)\n",
            "Skipping post 126t726 (already fetched)\n",
            "Skipping post j26bz0 (already fetched)\n",
            "Skipping post 1d5uid1 (already fetched)\n",
            "Skipping post 1bvlzii (already fetched)\n",
            "Skipping post 1f6qdn7 (already fetched)\n",
            "Skipping post sp0wz3 (already fetched)\n",
            "Skipping post iaym1i (already fetched)\n",
            "Skipping post uhrf44 (already fetched)\n",
            "Skipping post q401q9 (already fetched)\n",
            "Skipping post 1el72z8 (already fetched)\n",
            "Skipping post u092fb (already fetched)\n",
            "Skipping post tu2qsn (already fetched)\n",
            "Skipping post nrx4nz (already fetched)\n",
            "Skipping post nr7glk (already fetched)\n",
            "Skipping post vmtbso (already fetched)\n",
            "Skipping post kf59mh (already fetched)\n",
            "Skipping post vk2r1s (already fetched)\n",
            "Skipping post 18inwyi (already fetched)\n",
            "Skipping post u30kfr (already fetched)\n",
            "Skipping post u24dif (already fetched)\n",
            "Skipping post uotpec (already fetched)\n",
            "Skipping post 173b6zw (already fetched)\n",
            "Skipping post 1bc90v5 (already fetched)\n",
            "Skipping post 1byum4m (already fetched)\n",
            "Skipping post 1byk4g4 (already fetched)\n",
            "Skipping post utlbq5 (already fetched)\n",
            "Skipping post uuse0q (already fetched)\n",
            "Skipping post 1bdn0m7 (already fetched)\n",
            "Skipping post fjmihj (already fetched)\n",
            "Skipping post 19en27s (already fetched)\n",
            "Skipping post 1e101tn (already fetched)\n",
            "Skipping post 1fjwwgf (already fetched)\n",
            "Skipping post lewwbs (already fetched)\n",
            "Skipping post n00kg3 (already fetched)\n",
            "Skipping post mypnn9 (already fetched)\n",
            "Skipping post z6xfuh (already fetched)\n",
            "Skipping post av8jq5 (already fetched)\n",
            "Skipping post 1f0ddbo (already fetched)\n",
            "Skipping post nh891h (already fetched)\n",
            "Skipping post 16qznhl (already fetched)\n",
            "Skipping post 1eqgq48 (already fetched)\n",
            "Skipping post q9m6gk (already fetched)\n",
            "Skipping post nu735k (already fetched)\n",
            "Skipping post 16trfxv (already fetched)\n",
            "Skipping post 1cpav2l (already fetched)\n",
            "Skipping post ibbzbl (already fetched)\n",
            "Skipping post lol7bh (already fetched)\n",
            "Skipping post hhduw0 (already fetched)\n",
            "Skipping post ly2xde (already fetched)\n",
            "Skipping post 18ngj0u (already fetched)\n",
            "Skipping post e5xsvu (already fetched)\n",
            "Skipping post 16zivy3 (already fetched)\n",
            "Skipping post bn4e76 (already fetched)\n",
            "Skipping post cuzi41 (already fetched)\n",
            "Skipping post 121d4gz (already fetched)\n",
            "Skipping post 1cmtqk9 (already fetched)\n",
            "Skipping post gokjtg (already fetched)\n",
            "Skipping post 1b5rucx (already fetched)\n",
            "Skipping post 1d0pv41 (already fetched)\n",
            "Skipping post 59x1dg (already fetched)\n",
            "Skipping post 17nnv6c (already fetched)\n",
            "Skipping post xa5gyh (already fetched)\n",
            "Skipping post 1e3bql0 (already fetched)\n",
            "Skipping post 1allxoj (already fetched)\n",
            "Skipping post 1avyfc8 (already fetched)\n",
            "Skipping post oq9utm (already fetched)\n",
            "Skipping post 1bh0m9l (already fetched)\n",
            "Skipping post hd9ywx (already fetched)\n",
            "Skipping post 15vgfm9 (already fetched)\n",
            "Skipping post 1fdsi3v (already fetched)\n",
            "Skipping post yamacl (already fetched)\n",
            "Skipping post 7m4cpi (already fetched)\n",
            "Skipping post biv65o (already fetched)\n",
            "Skipping post kei1mm (already fetched)\n",
            "Skipping post 15l3wgu (already fetched)\n",
            "Skipping post ye4fko (already fetched)\n",
            "Skipping post hxijwg (already fetched)\n",
            "Skipping post 1e7prck (already fetched)\n",
            "Skipping post af9ycz (already fetched)\n",
            "Skipping post 8zi45c (already fetched)\n",
            "Skipping post hbgwio (already fetched)\n",
            "Skipping post fnx5cd (already fetched)\n",
            "Skipping post bca7xs (already fetched)\n",
            "Skipping post 15sg94r (already fetched)\n",
            "Skipping post 1bvc3h0 (already fetched)\n",
            "Skipping post 1dgwxn4 (already fetched)\n",
            "Skipping post av6cd9 (already fetched)\n",
            "Skipping post 1amyb9b (already fetched)\n",
            "Skipping post 1enfetg (already fetched)\n",
            "Skipping post 1bxfhx5 (already fetched)\n",
            "Skipping post dohaea (already fetched)\n",
            "Skipping post 1bgt0cm (already fetched)\n",
            "Skipping post vqs6iu (already fetched)\n",
            "Skipping post 1fivg2s (already fetched)\n",
            "Skipping post fsqb9t (already fetched)\n",
            "Skipping post cdiu3e (already fetched)\n",
            "Skipping post 1civxsv (already fetched)\n",
            "Skipping post bjot4n (already fetched)\n",
            "Skipping post qirwqd (already fetched)\n",
            "Skipping post qgxj1b (already fetched)\n",
            "Skipping post 9srjen (already fetched)\n",
            "Skipping post a0yvz2 (already fetched)\n",
            "Skipping post 1397o3z (already fetched)\n",
            "Skipping post 1cpzjwf (already fetched)\n",
            "Processed post ID: 1fxwgme\n",
            "Processed post ID: 1fu3x6q\n",
            "Skipping post 3q0xru (already fetched)\n",
            "Skipping post vrkfu4 (already fetched)\n",
            "Skipping post 1e3zrb3 (already fetched)\n",
            "Skipping post 1cvc0mn (already fetched)\n",
            "Skipping post w9249k (already fetched)\n",
            "Skipping post 18m16ez (already fetched)\n",
            "Skipping post 1brz54j (already fetched)\n",
            "Skipping post 16dpce1 (already fetched)\n",
            "Skipping post 1dn5z15 (already fetched)\n",
            "Skipping post 1f5pun6 (already fetched)\n",
            "Skipping post 1867duz (already fetched)\n",
            "Skipping post vldain (already fetched)\n",
            "Skipping post p45kt2 (already fetched)\n",
            "Skipping post 1evnbes (already fetched)\n",
            "Skipping post 1dbveuf (already fetched)\n",
            "Skipping post 1alftd2 (already fetched)\n",
            "Skipping post 1cd5jjb (already fetched)\n",
            "Skipping post 1dd02nr (already fetched)\n",
            "Skipping post 946rs4 (already fetched)\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# File to store previously fetched post IDs\n",
        "post_id_log = 'fetched_posts.txt'\n",
        "# File to store the last fetched timestamp\n",
        "last_timestamp_file = 'last_fetched_timestamp.txt'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Load previously fetched post IDs from the log file\n",
        "if os.path.exists(post_id_log):\n",
        "    with open(post_id_log, 'r') as f:\n",
        "        fetched_post_ids = set(f.strip() for f in f.readlines())\n",
        "else:\n",
        "    fetched_post_ids = set()\n",
        "\n",
        "# Load the last fetched timestamp\n",
        "if os.path.exists(last_timestamp_file):\n",
        "    with open(last_timestamp_file, 'r') as f:\n",
        "        last_fetched_timestamp = float(f.read().strip())\n",
        "else:\n",
        "    last_fetched_timestamp = 0  # Start from the beginning if the file does not exist\n",
        "\n",
        "# Open CSV file for appending (to avoid overwriting previous data)\n",
        "with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # If the CSV file is new, write the header row\n",
        "    if os.stat(csv_file).st_size == 0:\n",
        "        writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        with open(post_id_log, 'a') as id_log_file:\n",
        "            # Use 'OR' logic to search for the two topics\n",
        "            for topic in topics:\n",
        "                print(f\"Searching for topic: {topic}\")\n",
        "                for post in subreddit.search(topic, sort='new', limit=1000):  # Adjust limit as needed\n",
        "                    # Only consider posts newer than the last fetched timestamp\n",
        "                    if post.created_utc <= last_fetched_timestamp:\n",
        "                        print(f\"Skipping post {post.id} (not newer than last fetched)\")\n",
        "                        continue\n",
        "\n",
        "                    post_id = post.id\n",
        "\n",
        "                    # Skip posts already fetched\n",
        "                    if post_id in fetched_post_ids:\n",
        "                        print(f\"Skipping post {post_id} (already fetched)\")\n",
        "                        continue\n",
        "\n",
        "                    title = post.title\n",
        "                    author = str(post.author) if post.author else 'Unknown'\n",
        "                    score = post.score\n",
        "                    post_url = post.url\n",
        "                    text = post.selftext\n",
        "                    media_filename = ''\n",
        "                    has_media = 'No'\n",
        "                    timestamp = post.created_utc\n",
        "                    post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                    post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                    post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                    # Check if the post has an image or video\n",
        "                    if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "                    elif post.url.endswith(('.mp4', '.webm')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                        download_media(post.url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "\n",
        "                    # Write post data to CSV\n",
        "                    writer.writerow({\n",
        "                        'Post ID': post_id,\n",
        "                        'Title': title,\n",
        "                        'Author': author,\n",
        "                        'Score': score,\n",
        "                        'Post URL': post_url,\n",
        "                        'Text': text,\n",
        "                        'Media Filename': media_filename,\n",
        "                        'Has Media': has_media,\n",
        "                        'Topic': topic,\n",
        "                        'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                        'Timestamp': timestamp,\n",
        "                        'Date': post_date,\n",
        "                        'Month': post_month,\n",
        "                        'Year': post_year\n",
        "                    })\n",
        "\n",
        "                    print(f\"Processed post ID: {post_id}\")\n",
        "\n",
        "                    # Save the post ID to the log file\n",
        "                    id_log_file.write(f\"{post_id}\\n\")\n",
        "                    fetched_post_ids.add(post_id)\n",
        "\n",
        "                    # Update the last fetched timestamp\n",
        "                    last_fetched_timestamp = max(last_fetched_timestamp, timestamp)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Save the last fetched timestamp to a file\n",
        "with open(last_timestamp_file, 'w') as f:\n",
        "    f.write(str(last_fetched_timestamp))\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dseWkCuWoug6",
        "outputId": "7d2afc29-7a28-4843-e191-c5ba7a3a8d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Searching for topic: Is this a scam?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 1fow3eu (already fetched)\n",
            "Skipping post 1fovnon (already fetched)\n",
            "Skipping post 1fovcm2 (already fetched)\n",
            "Skipping post 1fou993 (already fetched)\n",
            "Skipping post 1fott3c (already fetched)\n",
            "Skipping post 1fotohk (already fetched)\n",
            "Skipping post 1fot6kx (already fetched)\n",
            "Skipping post 1foseuh (already fetched)\n",
            "Skipping post 1fosdal (already fetched)\n",
            "Skipping post 1forq8i (already fetched)\n",
            "Skipping post 1foqzrs (already fetched)\n",
            "Skipping post 1foqpwr (already fetched)\n",
            "Skipping post 1foq9r2 (already fetched)\n",
            "Skipping post 1fopppb (already fetched)\n",
            "Skipping post 1fopdi4 (already fetched)\n",
            "Skipping post 1fop8e5 (already fetched)\n",
            "Skipping post 1fop1fk (already fetched)\n",
            "Skipping post 1fongba (already fetched)\n",
            "Skipping post 1fong93 (already fetched)\n",
            "Skipping post 1fomvs0 (not newer than last fetched)\n",
            "Skipping post 1fom0n7 (not newer than last fetched)\n",
            "Skipping post 1folqaw (not newer than last fetched)\n",
            "Skipping post 1folnwn (not newer than last fetched)\n",
            "Skipping post 1fokptl (not newer than last fetched)\n",
            "Skipping post 1fokets (not newer than last fetched)\n",
            "Skipping post 1fojx51 (not newer than last fetched)\n",
            "Skipping post 1fojnxf (not newer than last fetched)\n",
            "Skipping post 1fojmjn (not newer than last fetched)\n",
            "Skipping post 1fojgpc (not newer than last fetched)\n",
            "Skipping post 1fojaws (not newer than last fetched)\n",
            "Skipping post 1foj250 (not newer than last fetched)\n",
            "Skipping post 1foij1e (not newer than last fetched)\n",
            "Skipping post 1foh2y3 (not newer than last fetched)\n",
            "Skipping post 1fogrrq (not newer than last fetched)\n",
            "Skipping post 1fogo3s (not newer than last fetched)\n",
            "Skipping post 1fogn81 (not newer than last fetched)\n",
            "Skipping post 1fogfpf (not newer than last fetched)\n",
            "Skipping post 1fogehh (not newer than last fetched)\n",
            "Skipping post 1fog639 (not newer than last fetched)\n",
            "Skipping post 1fofxep (not newer than last fetched)\n",
            "Skipping post 1fofqr7 (not newer than last fetched)\n",
            "Skipping post 1fofk76 (not newer than last fetched)\n",
            "Skipping post 1fofjkf (not newer than last fetched)\n",
            "Skipping post 1fofddq (not newer than last fetched)\n",
            "Skipping post 1fof1xu (not newer than last fetched)\n",
            "Skipping post 1foelfw (not newer than last fetched)\n",
            "Skipping post 1foekj6 (not newer than last fetched)\n",
            "Skipping post 1fodziw (not newer than last fetched)\n",
            "Skipping post 1focvuj (not newer than last fetched)\n",
            "Skipping post 1foc5pt (not newer than last fetched)\n",
            "Skipping post 1fobc9e (not newer than last fetched)\n",
            "Skipping post 1fo9j03 (not newer than last fetched)\n",
            "Skipping post 1fo97so (not newer than last fetched)\n",
            "Skipping post 1fo92yz (not newer than last fetched)\n",
            "Skipping post 1fo8r45 (not newer than last fetched)\n",
            "Skipping post 1fo7v81 (not newer than last fetched)\n",
            "Skipping post 1fo7fsh (not newer than last fetched)\n",
            "Skipping post 1fo77go (not newer than last fetched)\n",
            "Skipping post 1fo6zgf (not newer than last fetched)\n",
            "Skipping post 1fo6u6y (not newer than last fetched)\n",
            "Skipping post 1fo5fv6 (not newer than last fetched)\n",
            "Skipping post 1fo5cvt (not newer than last fetched)\n",
            "Skipping post 1fo4xn7 (not newer than last fetched)\n",
            "Skipping post 1fo4sxe (not newer than last fetched)\n",
            "Skipping post 1fo4msb (not newer than last fetched)\n",
            "Skipping post 1fo4hop (not newer than last fetched)\n",
            "Skipping post 1fo4fza (not newer than last fetched)\n",
            "Skipping post 1fo4e2v (not newer than last fetched)\n",
            "Skipping post 1fo49c2 (not newer than last fetched)\n",
            "Skipping post 1fo2kak (not newer than last fetched)\n",
            "Skipping post 1fo2ie2 (not newer than last fetched)\n",
            "Skipping post 1fo2d3a (not newer than last fetched)\n",
            "Skipping post 1fo25eu (not newer than last fetched)\n",
            "Skipping post 1fo1var (not newer than last fetched)\n",
            "Skipping post 1fo0zjd (not newer than last fetched)\n",
            "Skipping post 1fo0qyu (not newer than last fetched)\n",
            "Skipping post 1fo0qwz (not newer than last fetched)\n",
            "Skipping post 1fo0g4x (not newer than last fetched)\n",
            "Skipping post 1fnzlb9 (not newer than last fetched)\n",
            "Skipping post 1fnzkmp (not newer than last fetched)\n",
            "Skipping post 1fnzk5b (not newer than last fetched)\n",
            "Skipping post 1fnzalf (not newer than last fetched)\n",
            "Skipping post 1fnz7y1 (not newer than last fetched)\n",
            "Skipping post 1fnyqqu (not newer than last fetched)\n",
            "Skipping post 1fnxy11 (not newer than last fetched)\n",
            "Skipping post 1fnxuzg (not newer than last fetched)\n",
            "Skipping post 1fnxawu (not newer than last fetched)\n",
            "Skipping post 1fnx6yn (not newer than last fetched)\n",
            "Skipping post 1fnx1d5 (not newer than last fetched)\n",
            "Skipping post 1fnw6ei (not newer than last fetched)\n",
            "Skipping post 1fnw11s (not newer than last fetched)\n",
            "Skipping post 1fnvsb3 (not newer than last fetched)\n",
            "Skipping post 1fnvnin (not newer than last fetched)\n",
            "Skipping post 1fnvid7 (not newer than last fetched)\n",
            "Skipping post 1fnvgyu (not newer than last fetched)\n",
            "Skipping post 1fnv9w3 (not newer than last fetched)\n",
            "Skipping post 1fnun22 (not newer than last fetched)\n",
            "Skipping post 1fntyvx (not newer than last fetched)\n",
            "Skipping post 1fntuqf (not newer than last fetched)\n",
            "Skipping post 1fntrn8 (not newer than last fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 1fnsubz (not newer than last fetched)\n",
            "Skipping post 1fnsnom (not newer than last fetched)\n",
            "Skipping post 1fnsgqq (not newer than last fetched)\n",
            "Skipping post 1fnscls (not newer than last fetched)\n",
            "Skipping post 1fnr5ur (not newer than last fetched)\n",
            "Skipping post 1fnqxsf (not newer than last fetched)\n",
            "Skipping post 1fnqtta (not newer than last fetched)\n",
            "Skipping post 1fnqn0u (not newer than last fetched)\n",
            "Skipping post 1fnqbzn (not newer than last fetched)\n",
            "Skipping post 1fnqaym (not newer than last fetched)\n",
            "Skipping post 1fnpzqw (not newer than last fetched)\n",
            "Skipping post 1fnpv8r (not newer than last fetched)\n",
            "Skipping post 1fnpsfv (not newer than last fetched)\n",
            "Skipping post 1fnoq5o (not newer than last fetched)\n",
            "Skipping post 1fnoq5a (not newer than last fetched)\n",
            "Skipping post 1fnody7 (not newer than last fetched)\n",
            "Skipping post 1fnnl75 (not newer than last fetched)\n",
            "Skipping post 1fnn5th (not newer than last fetched)\n",
            "Skipping post 1fnn4ib (not newer than last fetched)\n",
            "Skipping post 1fnmmr5 (not newer than last fetched)\n",
            "Skipping post 1fnma38 (not newer than last fetched)\n",
            "Skipping post 1fnll3y (not newer than last fetched)\n",
            "Skipping post 1fnlc22 (not newer than last fetched)\n",
            "Skipping post 1fnkx6x (not newer than last fetched)\n",
            "Skipping post 1fnkth8 (not newer than last fetched)\n",
            "Skipping post 1fnkiei (not newer than last fetched)\n",
            "Skipping post 1fnk4bo (not newer than last fetched)\n",
            "Skipping post 1fnjs2m (not newer than last fetched)\n",
            "Skipping post 1fnjo3h (not newer than last fetched)\n",
            "Skipping post 1fnjeuz (not newer than last fetched)\n",
            "Skipping post 1fnizmt (not newer than last fetched)\n",
            "Skipping post 1fnhoqr (not newer than last fetched)\n",
            "Skipping post 1fnh12j (not newer than last fetched)\n",
            "Skipping post 1fngu0q (not newer than last fetched)\n",
            "Skipping post 1fngdhw (not newer than last fetched)\n",
            "Skipping post 1fnehuu (not newer than last fetched)\n",
            "Skipping post 1fndmjr (not newer than last fetched)\n",
            "Skipping post 1fncn4f (not newer than last fetched)\n",
            "Skipping post 1fnc8wo (not newer than last fetched)\n",
            "Skipping post 1fnc7n2 (not newer than last fetched)\n",
            "Skipping post 1fnc48x (not newer than last fetched)\n",
            "Skipping post 1fnbfw5 (not newer than last fetched)\n",
            "Skipping post 1fnay9k (not newer than last fetched)\n",
            "Skipping post 1fnaxc8 (not newer than last fetched)\n",
            "Skipping post 1fnausr (not newer than last fetched)\n",
            "Skipping post 1fn9jp7 (not newer than last fetched)\n",
            "Skipping post 1fn8foc (not newer than last fetched)\n",
            "Skipping post 1fn5m6d (not newer than last fetched)\n",
            "Skipping post 1fn55cq (not newer than last fetched)\n",
            "Skipping post 1fn4dcr (not newer than last fetched)\n",
            "Skipping post 1fn3z92 (not newer than last fetched)\n",
            "Skipping post 1fn3k6h (not newer than last fetched)\n",
            "Skipping post 1fn3k1n (not newer than last fetched)\n",
            "Skipping post 1fn3hta (not newer than last fetched)\n",
            "Skipping post 1fn1o4t (not newer than last fetched)\n",
            "Skipping post 1fn1enj (not newer than last fetched)\n",
            "Skipping post 1fn0rr5 (not newer than last fetched)\n",
            "Skipping post 1fn0aea (not newer than last fetched)\n",
            "Skipping post 1fmyef6 (not newer than last fetched)\n",
            "Skipping post 1fmxkk2 (not newer than last fetched)\n",
            "Skipping post 1fmwlbl (not newer than last fetched)\n",
            "Skipping post 1fmvxta (not newer than last fetched)\n",
            "Skipping post 1fmvlqn (not newer than last fetched)\n",
            "Skipping post 1fmtn0f (not newer than last fetched)\n",
            "Skipping post 1fmtarl (not newer than last fetched)\n",
            "Skipping post 1fmt1l6 (not newer than last fetched)\n",
            "Skipping post 1fmshw8 (not newer than last fetched)\n",
            "Skipping post 1fmshms (not newer than last fetched)\n",
            "Skipping post 1fmraqx (not newer than last fetched)\n",
            "Skipping post 1fmmks3 (not newer than last fetched)\n",
            "Skipping post 1fmksr6 (not newer than last fetched)\n",
            "Skipping post 1fmkmqk (not newer than last fetched)\n",
            "Skipping post 1fmkfqa (not newer than last fetched)\n",
            "Skipping post 1fmk37d (not newer than last fetched)\n",
            "Skipping post 1fmik28 (not newer than last fetched)\n",
            "Skipping post 1fmideq (not newer than last fetched)\n",
            "Skipping post 1fmgowc (not newer than last fetched)\n",
            "Skipping post 1fmgbz8 (not newer than last fetched)\n",
            "Skipping post 1fmfgf3 (not newer than last fetched)\n",
            "Skipping post 1fmfg3s (not newer than last fetched)\n",
            "Skipping post 1fmf34w (not newer than last fetched)\n",
            "Skipping post 1fmf0dp (not newer than last fetched)\n",
            "Skipping post 1fmehqo (not newer than last fetched)\n",
            "Skipping post 1fmed0y (not newer than last fetched)\n",
            "Skipping post 1fmde1z (not newer than last fetched)\n",
            "Skipping post 1fmdc9a (not newer than last fetched)\n",
            "Skipping post 1fmco21 (not newer than last fetched)\n",
            "Skipping post 1fmcko8 (not newer than last fetched)\n",
            "Skipping post 1fmaxer (not newer than last fetched)\n",
            "Skipping post 1fmafj6 (not newer than last fetched)\n",
            "Skipping post 1fma2hb (not newer than last fetched)\n",
            "Skipping post 1fma1wc (not newer than last fetched)\n",
            "Skipping post 1fm9u28 (not newer than last fetched)\n",
            "Skipping post 1fm9swo (not newer than last fetched)\n",
            "Skipping post 1fm9807 (not newer than last fetched)\n",
            "Skipping post 1fm97r3 (not newer than last fetched)\n",
            "Skipping post 1fm8ses (not newer than last fetched)\n",
            "Skipping post 1fm8hjw (not newer than last fetched)\n",
            "Skipping post 1fm89ix (not newer than last fetched)\n",
            "Skipping post 1fm7p7z (not newer than last fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 1fm7lg3 (not newer than last fetched)\n",
            "Skipping post 1fm6vxa (not newer than last fetched)\n",
            "Skipping post 1fm6ge3 (not newer than last fetched)\n",
            "Skipping post 1fm645a (not newer than last fetched)\n",
            "Skipping post 1fm5lgt (not newer than last fetched)\n",
            "Skipping post 1fm4vva (not newer than last fetched)\n",
            "Skipping post 1fm4mt8 (not newer than last fetched)\n",
            "Skipping post 1fm3itb (not newer than last fetched)\n",
            "Skipping post 1fm3hnq (not newer than last fetched)\n",
            "Skipping post 1fm2tno (not newer than last fetched)\n",
            "Skipping post 1fm22w5 (not newer than last fetched)\n",
            "Skipping post 1fm1gkc (not newer than last fetched)\n",
            "Skipping post 1fm0wa2 (not newer than last fetched)\n",
            "Skipping post 1fm0o1d (not newer than last fetched)\n",
            "Skipping post 1fm0gqd (not newer than last fetched)\n",
            "Skipping post 1flwxjy (not newer than last fetched)\n",
            "Skipping post 1flv6rm (not newer than last fetched)\n",
            "Skipping post 1flulb3 (not newer than last fetched)\n",
            "Skipping post 1fluaa5 (not newer than last fetched)\n",
            "Skipping post 1flthg2 (not newer than last fetched)\n",
            "Skipping post 1flruqm (not newer than last fetched)\n",
            "Skipping post 1flrrjq (not newer than last fetched)\n",
            "Skipping post 1flq9em (not newer than last fetched)\n",
            "Skipping post 1flp1od (not newer than last fetched)\n",
            "Skipping post 1flnxu5 (not newer than last fetched)\n",
            "Skipping post 1fln2fc (not newer than last fetched)\n",
            "Skipping post 1flmfum (not newer than last fetched)\n",
            "Skipping post 1fllnrl (not newer than last fetched)\n",
            "Skipping post 1flldu0 (not newer than last fetched)\n",
            "Skipping post 1fll9mj (not newer than last fetched)\n",
            "Skipping post 1fll83a (not newer than last fetched)\n",
            "Skipping post 1flkqii (not newer than last fetched)\n",
            "Skipping post 1flkm1a (not newer than last fetched)\n",
            "Skipping post 1flk4fd (not newer than last fetched)\n",
            "Skipping post 1fljt9e (not newer than last fetched)\n",
            "Skipping post 1flhvtw (not newer than last fetched)\n",
            "Skipping post 1flhm9o (not newer than last fetched)\n",
            "Skipping post 1flhld4 (not newer than last fetched)\n",
            "Skipping post 1flgyu5 (not newer than last fetched)\n",
            "Skipping post 1flgleu (not newer than last fetched)\n",
            "Skipping post 1flg5dd (not newer than last fetched)\n",
            "Skipping post 1flfpiz (not newer than last fetched)\n",
            "Skipping post 1flfkkb (not newer than last fetched)\n",
            "Skipping post 1flfhj4 (not newer than last fetched)\n",
            "Skipping post 1flf6ad (not newer than last fetched)\n",
            "Skipping post 1flevcp (not newer than last fetched)\n",
            "Skipping post 1fle1t1 (not newer than last fetched)\n",
            "Skipping post 1flc55o (not newer than last fetched)\n",
            "Searching for topic: Victim of a scam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 1foqnjl (already fetched)\n",
            "Skipping post 1fogn81 (not newer than last fetched)\n",
            "Skipping post 1fnkth8 (not newer than last fetched)\n",
            "Skipping post 1fnc7n2 (not newer than last fetched)\n",
            "Skipping post 1fmksr6 (not newer than last fetched)\n",
            "Skipping post 1fmed0y (not newer than last fetched)\n",
            "Skipping post 1fm9swo (not newer than last fetched)\n",
            "Skipping post 1flydep (not newer than last fetched)\n",
            "Skipping post 1flmwhd (not newer than last fetched)\n",
            "Skipping post 1flgyu5 (not newer than last fetched)\n",
            "Skipping post 1fl34e8 (not newer than last fetched)\n",
            "Skipping post 1fkz6p4 (not newer than last fetched)\n",
            "Skipping post 1fkxm79 (not newer than last fetched)\n",
            "Skipping post 1fkrocp (not newer than last fetched)\n",
            "Skipping post 1fkalae (not newer than last fetched)\n",
            "Skipping post 1fk67t5 (not newer than last fetched)\n",
            "Skipping post 1fjywsc (not newer than last fetched)\n",
            "Skipping post 1fjwwgf (not newer than last fetched)\n",
            "Skipping post 1fj46kb (not newer than last fetched)\n",
            "Skipping post 1fixvx2 (not newer than last fetched)\n",
            "Skipping post 1fivg2s (not newer than last fetched)\n",
            "Skipping post 1fi0zlo (not newer than last fetched)\n",
            "Skipping post 1fhub0n (not newer than last fetched)\n",
            "Skipping post 1fhsh91 (not newer than last fetched)\n",
            "Skipping post 1fhn5zl (not newer than last fetched)\n",
            "Skipping post 1fgy9fj (not newer than last fetched)\n",
            "Skipping post 1fgtati (not newer than last fetched)\n",
            "Skipping post 1fgjuog (not newer than last fetched)\n",
            "Skipping post 1fgjkvg (not newer than last fetched)\n",
            "Skipping post 1fgijgd (not newer than last fetched)\n",
            "Skipping post 1ffto2b (not newer than last fetched)\n",
            "Skipping post 1fft3ki (not newer than last fetched)\n",
            "Skipping post 1ffngf2 (not newer than last fetched)\n",
            "Skipping post 1ff7nli (not newer than last fetched)\n",
            "Skipping post 1ff15wv (not newer than last fetched)\n",
            "Skipping post 1fezz2s (not newer than last fetched)\n",
            "Skipping post 1feypzf (not newer than last fetched)\n",
            "Skipping post 1fenh51 (not newer than last fetched)\n",
            "Skipping post 1fek859 (not newer than last fetched)\n",
            "Skipping post 1fdrhhr (not newer than last fetched)\n",
            "Skipping post 1fddatq (not newer than last fetched)\n",
            "Skipping post 1fd0v85 (not newer than last fetched)\n",
            "Skipping post 1fcs759 (not newer than last fetched)\n",
            "Skipping post 1fc6lmi (not newer than last fetched)\n",
            "Skipping post 1fbzs2x (not newer than last fetched)\n",
            "Skipping post 1fb6eyw (not newer than last fetched)\n",
            "Skipping post 1fb3sk1 (not newer than last fetched)\n",
            "Skipping post 1fawuqa (not newer than last fetched)\n",
            "Skipping post 1f9w6nq (not newer than last fetched)\n",
            "Skipping post 1f9w56s (not newer than last fetched)\n",
            "Skipping post 1f90te6 (not newer than last fetched)\n",
            "Skipping post 1f8zh77 (not newer than last fetched)\n",
            "Skipping post 1f8nsu8 (not newer than last fetched)\n",
            "Skipping post 1f8d685 (not newer than last fetched)\n",
            "Skipping post 1f7vnvk (not newer than last fetched)\n",
            "Skipping post 1f7myor (not newer than last fetched)\n",
            "Skipping post 1f7idao (not newer than last fetched)\n",
            "Skipping post 1f7cbh7 (not newer than last fetched)\n",
            "Skipping post 1f6qdn7 (not newer than last fetched)\n",
            "Skipping post 1f5pun6 (not newer than last fetched)\n",
            "Skipping post 1f55n8n (not newer than last fetched)\n",
            "Skipping post 1f4qeyk (not newer than last fetched)\n",
            "Skipping post 1f4ck8e (not newer than last fetched)\n",
            "Skipping post 1f3ox3b (not newer than last fetched)\n",
            "Skipping post 1f3guux (not newer than last fetched)\n",
            "Skipping post 1f373od (not newer than last fetched)\n",
            "Skipping post 1f2qnjh (not newer than last fetched)\n",
            "Skipping post 1f24nij (not newer than last fetched)\n",
            "Skipping post 1f22hsx (not newer than last fetched)\n",
            "Skipping post 1f21e2k (not newer than last fetched)\n",
            "Skipping post 1f1zajz (not newer than last fetched)\n",
            "Skipping post 1f184eh (not newer than last fetched)\n",
            "Skipping post 1f0e2cj (not newer than last fetched)\n",
            "Skipping post 1ezuhl7 (not newer than last fetched)\n",
            "Skipping post 1eypqn0 (not newer than last fetched)\n",
            "Skipping post 1eyklel (not newer than last fetched)\n",
            "Skipping post 1ey0ld8 (not newer than last fetched)\n",
            "Skipping post 1exh7sj (not newer than last fetched)\n",
            "Skipping post 1exex2m (not newer than last fetched)\n",
            "Skipping post 1ewoo6w (not newer than last fetched)\n",
            "Skipping post 1ewhr9n (not newer than last fetched)\n",
            "Skipping post 1ewhjbd (not newer than last fetched)\n",
            "Skipping post 1ew6s7z (not newer than last fetched)\n",
            "Skipping post 1evnbes (not newer than last fetched)\n",
            "Skipping post 1evbq5t (not newer than last fetched)\n",
            "Skipping post 1ev9nt8 (not newer than last fetched)\n",
            "Skipping post 1ev8f8a (not newer than last fetched)\n",
            "Skipping post 1ev1vtx (not newer than last fetched)\n",
            "Skipping post 1eubil5 (not newer than last fetched)\n",
            "Skipping post 1etrkoz (not newer than last fetched)\n",
            "Skipping post 1et95gv (not newer than last fetched)\n",
            "Skipping post 1ess3jy (not newer than last fetched)\n",
            "Skipping post 1esnpwg (not newer than last fetched)\n",
            "Skipping post 1eslc0y (not newer than last fetched)\n",
            "Skipping post 1esayti (not newer than last fetched)\n",
            "Skipping post 1es8xm1 (not newer than last fetched)\n",
            "Skipping post 1erw9oo (not newer than last fetched)\n",
            "Skipping post 1erw3tm (not newer than last fetched)\n",
            "Skipping post 1erghuj (not newer than last fetched)\n",
            "Skipping post 1erbc2v (not newer than last fetched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping post 1er48aw (not newer than last fetched)\n",
            "Skipping post 1eqzsnk (not newer than last fetched)\n",
            "Skipping post 1eqnkfe (not newer than last fetched)\n",
            "Skipping post 1eqnd5q (not newer than last fetched)\n",
            "Skipping post 1eqgq48 (not newer than last fetched)\n",
            "Skipping post 1epnvtv (not newer than last fetched)\n",
            "Skipping post 1epgrhx (not newer than last fetched)\n",
            "Skipping post 1epgpfd (not newer than last fetched)\n",
            "Skipping post 1eoni1m (not newer than last fetched)\n",
            "Skipping post 1eob7lf (not newer than last fetched)\n",
            "Skipping post 1eoawm9 (not newer than last fetched)\n",
            "Skipping post 1eo9hwh (not newer than last fetched)\n",
            "Skipping post 1eo0xef (not newer than last fetched)\n",
            "Skipping post 1enyimv (not newer than last fetched)\n",
            "Skipping post 1enjflk (not newer than last fetched)\n",
            "Skipping post 1eng8mx (not newer than last fetched)\n",
            "Skipping post 1enfetg (not newer than last fetched)\n",
            "Skipping post 1emw4yt (not newer than last fetched)\n",
            "Skipping post 1emstfn (not newer than last fetched)\n",
            "Skipping post 1em9fla (not newer than last fetched)\n",
            "Skipping post 1eln6q6 (not newer than last fetched)\n",
            "Skipping post 1elka89 (not newer than last fetched)\n",
            "Skipping post 1elir60 (not newer than last fetched)\n",
            "Skipping post 1el72z8 (not newer than last fetched)\n",
            "Skipping post 1ekzieh (not newer than last fetched)\n",
            "Skipping post 1ekxftr (not newer than last fetched)\n",
            "Skipping post 1ekwok0 (not newer than last fetched)\n",
            "Skipping post 1ekt7qk (not newer than last fetched)\n",
            "Skipping post 1eknhrh (not newer than last fetched)\n",
            "Skipping post 1ejnvlf (not newer than last fetched)\n",
            "Skipping post 1ejiwxj (not newer than last fetched)\n",
            "Skipping post 1ejiirp (not newer than last fetched)\n",
            "Skipping post 1ejcs61 (not newer than last fetched)\n",
            "Skipping post 1ej5o66 (not newer than last fetched)\n",
            "Skipping post 1eiy86q (not newer than last fetched)\n",
            "Skipping post 1einef0 (not newer than last fetched)\n",
            "Skipping post 1eihy7b (not newer than last fetched)\n",
            "Skipping post 1eifok8 (not newer than last fetched)\n",
            "Skipping post 1ei8s38 (not newer than last fetched)\n",
            "Skipping post 1ehn3sm (not newer than last fetched)\n",
            "Skipping post 1ehmtpe (not newer than last fetched)\n",
            "Skipping post 1eh7x1a (not newer than last fetched)\n",
            "Skipping post 1egsblm (not newer than last fetched)\n",
            "Skipping post 1egd6mx (not newer than last fetched)\n",
            "Skipping post 1efz9fn (not newer than last fetched)\n",
            "Skipping post 1efwcvz (not newer than last fetched)\n",
            "Skipping post 1eft5vw (not newer than last fetched)\n",
            "Skipping post 1efn073 (not newer than last fetched)\n",
            "Skipping post 1eekzgk (not newer than last fetched)\n",
            "Skipping post 1eeekin (not newer than last fetched)\n",
            "Skipping post 1eec5u2 (not newer than last fetched)\n",
            "Skipping post 1edscan (not newer than last fetched)\n",
            "Skipping post 1edqcyi (not newer than last fetched)\n",
            "Skipping post 1edovyw (not newer than last fetched)\n",
            "Skipping post 1ed4w4j (not newer than last fetched)\n",
            "Skipping post 1ed1lh5 (not newer than last fetched)\n",
            "Skipping post 1ecpgb1 (not newer than last fetched)\n",
            "Skipping post 1ecn85w (not newer than last fetched)\n",
            "Skipping post 1ecjjrm (not newer than last fetched)\n",
            "Skipping post 1ebzvrs (not newer than last fetched)\n",
            "Skipping post 1ebrq0q (not newer than last fetched)\n",
            "Skipping post 1ebrnge (not newer than last fetched)\n",
            "Skipping post 1ebn9iz (not newer than last fetched)\n",
            "Skipping post 1ebl0mb (not newer than last fetched)\n",
            "Skipping post 1ebhq9b (not newer than last fetched)\n",
            "Skipping post 1ea7abm (not newer than last fetched)\n",
            "Skipping post 1e9ql35 (not newer than last fetched)\n",
            "Skipping post 1e9qdzm (not newer than last fetched)\n",
            "Skipping post 1e8gh6z (not newer than last fetched)\n",
            "Skipping post 1e880si (not newer than last fetched)\n",
            "Skipping post 1e7prck (not newer than last fetched)\n",
            "Skipping post 1e7olwb (not newer than last fetched)\n",
            "Skipping post 1e7k7xg (not newer than last fetched)\n",
            "Skipping post 1e6gckg (not newer than last fetched)\n",
            "Skipping post 1e6a4lx (not newer than last fetched)\n",
            "Skipping post 1e5id8u (not newer than last fetched)\n",
            "Skipping post 1e53gyq (not newer than last fetched)\n",
            "Skipping post 1e4x5jw (not newer than last fetched)\n",
            "Skipping post 1e4oely (not newer than last fetched)\n",
            "Skipping post 1e4lo96 (not newer than last fetched)\n",
            "Skipping post 1e4jeyg (not newer than last fetched)\n",
            "Skipping post 1e4eete (not newer than last fetched)\n",
            "Skipping post 1e4b4xb (not newer than last fetched)\n",
            "Skipping post 1e49cjm (not newer than last fetched)\n",
            "Skipping post 1e3zrb3 (not newer than last fetched)\n",
            "Skipping post 1e3t431 (not newer than last fetched)\n",
            "Skipping post 1e3d8sx (not newer than last fetched)\n",
            "Skipping post 1e2rkwu (not newer than last fetched)\n",
            "Skipping post 1e2m7de (not newer than last fetched)\n",
            "Skipping post 1e2gggd (not newer than last fetched)\n",
            "Skipping post 1e1srp4 (not newer than last fetched)\n",
            "Skipping post 1e1pa1w (not newer than last fetched)\n",
            "Skipping post 1e1iibs (not newer than last fetched)\n",
            "Skipping post 1e14irq (not newer than last fetched)\n",
            "Skipping post 1e138b4 (not newer than last fetched)\n",
            "Skipping post 1e101tn (not newer than last fetched)\n",
            "Skipping post 1e0u4c5 (not newer than last fetched)\n",
            "Skipping post 1e0j4vr (not newer than last fetched)\n",
            "Skipping post 1e0h92q (not newer than last fetched)\n",
            "Skipping post 1e0grhh (not newer than last fetched)\n",
            "Skipping post 1e0ejjs (not newer than last fetched)\n",
            "Skipping post 1e065za (not newer than last fetched)\n",
            "Skipping post 1dzyzs5 (not newer than last fetched)\n",
            "Skipping post 1dzu5hj (not newer than last fetched)\n",
            "Skipping post 1dzbweq (not newer than last fetched)\n",
            "Skipping post 1dz8a85 (not newer than last fetched)\n",
            "Skipping post 1dyrv0m (not newer than last fetched)\n",
            "Skipping post 1dyd8d5 (not newer than last fetched)\n",
            "Skipping post 1dy0b6v (not newer than last fetched)\n",
            "Skipping post 1dxqgup (not newer than last fetched)\n",
            "Skipping post 1dxlq7t (not newer than last fetched)\n",
            "Skipping post 1dxgjia (not newer than last fetched)\n",
            "Skipping post 1dxd2tw (not newer than last fetched)\n",
            "Skipping post 1dxcalr (not newer than last fetched)\n",
            "Skipping post 1dx9udy (not newer than last fetched)\n",
            "Skipping post 1dwvnuc (not newer than last fetched)\n",
            "Skipping post 1dws35h (not newer than last fetched)\n",
            "Skipping post 1dwj3wc (not newer than last fetched)\n",
            "Skipping post 1dwdu6r (not newer than last fetched)\n",
            "Skipping post 1dug9ja (not newer than last fetched)\n",
            "Skipping post 1du9r81 (not newer than last fetched)\n",
            "Skipping post 1du8s4x (not newer than last fetched)\n",
            "Skipping post 1dtzsjo (not newer than last fetched)\n",
            "Skipping post 1dtngzt (not newer than last fetched)\n",
            "Skipping post 1dt2le0 (not newer than last fetched)\n",
            "Skipping post 1dstlb1 (not newer than last fetched)\n",
            "Skipping post 1dst50r (not newer than last fetched)\n",
            "Skipping post 1dspjel (not newer than last fetched)\n",
            "Skipping post 1dsmg6h (not newer than last fetched)\n",
            "Skipping post 1dsitc3 (not newer than last fetched)\n",
            "Skipping post 1ds7gtb (not newer than last fetched)\n",
            "Skipping post 1drownw (not newer than last fetched)\n",
            "Skipping post 1drk1ls (not newer than last fetched)\n",
            "Skipping post 1dqp1hf (not newer than last fetched)\n",
            "Skipping post 1dqgnee (not newer than last fetched)\n",
            "Skipping post 1dpj297 (not newer than last fetched)\n",
            "Skipping post 1dpf48g (not newer than last fetched)\n",
            "Skipping post 1dpeoka (not newer than last fetched)\n",
            "Skipping post 1dp9ozu (not newer than last fetched)\n",
            "Skipping post 1dood53 (not newer than last fetched)\n",
            "Skipping post 1dolib8 (not newer than last fetched)\n",
            "Skipping post 1dojt6x (not newer than last fetched)\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlBIPQxoI9p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Reddit client\n",
        "reddit = praw.Reddit(\n",
        "    client_id='iabZgypbw10fUa3b6-EhxQ',        # Replace with your client ID\n",
        "    client_secret='amMHnxuH9-hiotn7vDMp4VmrIOR57w', # Replace with your client secret\n",
        "    user_agent='SCVI_data'        # Replace with a user agent string, e.g., 'my_reddit_scraper'\n",
        ")\n",
        "\n",
        "# Specify the subreddit you want to extract posts from\n",
        "subreddit = reddit.subreddit('Scams')  # Replace with the name of the subreddit\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# File to store previously fetched post IDs\n",
        "post_id_log = 'fetched_posts.txt'\n",
        "# File to store the last fetched timestamp\n",
        "last_timestamp_file = 'last_fetched_timestamp.txt'\n",
        "\n",
        "# Function to download media and save with the post ID as the filename\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Load previously fetched post IDs from the log file\n",
        "if os.path.exists(post_id_log):\n",
        "    with open(post_id_log, 'r') as f:\n",
        "        fetched_post_ids = set(f.strip() for f in f.readlines())\n",
        "else:\n",
        "    fetched_post_ids = set()\n",
        "\n",
        "# Load the last fetched timestamp\n",
        "if os.path.exists(last_timestamp_file):\n",
        "    with open(last_timestamp_file, 'r') as f:\n",
        "        last_fetched_timestamp = float(f.read().strip())\n",
        "else:\n",
        "    last_fetched_timestamp = float('inf')  # Start from the far future if the file does not exist\n",
        "\n",
        "# Open CSV file for appending (to avoid overwriting previous data)\n",
        "with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # If the CSV file is new, write the header row\n",
        "    if os.stat(csv_file).st_size == 0:\n",
        "        writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "\n",
        "    try:\n",
        "        with open(post_id_log, 'a') as id_log_file:\n",
        "            # Use 'OR' logic to search for the two topics\n",
        "            for topic in topics:\n",
        "                print(f\"Searching for topic: {topic}\")\n",
        "\n",
        "                # Fetch older posts until we reach the last fetched timestamp\n",
        "                after_timestamp = None\n",
        "                total_posts_fetched = 0\n",
        "\n",
        "                while True:\n",
        "                    # Fetch posts\n",
        "                    posts = subreddit.search(topic, sort='new', limit=1000, time_filter='all', params={'before': after_timestamp})\n",
        "\n",
        "                    if not posts:\n",
        "                        print(\"No more posts to fetch.\")\n",
        "                        break\n",
        "\n",
        "                    for post in posts:\n",
        "                        if post.created_utc < last_fetched_timestamp:\n",
        "                            print(f\"Stopping search at post {post.id} (older than last fetched)\")\n",
        "                            break\n",
        "\n",
        "                        # Skip posts already fetched\n",
        "                        if post.id in fetched_post_ids:\n",
        "                            print(f\"Skipping post {post.id} (already fetched)\")\n",
        "                            continue\n",
        "\n",
        "                        title = post.title\n",
        "                        author = str(post.author) if post.author else 'Unknown'\n",
        "                        score = post.score\n",
        "                        post_url = post.url\n",
        "                        text = post.selftext\n",
        "                        media_filename = ''\n",
        "                        has_media = 'No'\n",
        "                        timestamp = post.created_utc\n",
        "                        post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                        post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                        post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                        # Check if the post has an image or video\n",
        "                        if post.url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                            media_filename = os.path.join('media', f\"{post.id}_image{os.path.splitext(post.url)[1]}\")\n",
        "                            download_media(post.url, media_filename)\n",
        "                            has_media = 'Yes'\n",
        "                        elif post.url.endswith(('.mp4', '.webm')):\n",
        "                            media_filename = os.path.join('media', f\"{post.id}_video{os.path.splitext(post.url)[1]}\")\n",
        "                            download_media(post.url, media_filename)\n",
        "                            has_media = 'Yes'\n",
        "\n",
        "                        # Write post data to CSV\n",
        "                        writer.writerow({\n",
        "                            'Post ID': post.id,\n",
        "                            'Title': title,\n",
        "                            'Author': author,\n",
        "                            'Score': score,\n",
        "                            'Post URL': post_url,\n",
        "                            'Text': text,\n",
        "                            'Media Filename': media_filename,\n",
        "                            'Has Media': has_media,\n",
        "                            'Topic': topic,\n",
        "                            'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                            'Timestamp': timestamp,\n",
        "                            'Date': post_date,\n",
        "                            'Month': post_month,\n",
        "                            'Year': post_year\n",
        "                        })\n",
        "\n",
        "                        print(f\"Processed post ID: {post.id}\")\n",
        "\n",
        "                        # Save the post ID to the log file\n",
        "                        id_log_file.write(f\"{post.id}\\n\")\n",
        "                        fetched_post_ids.add(post.id)\n",
        "\n",
        "                        # Update the last fetched timestamp\n",
        "                        last_fetched_timestamp = min(last_fetched_timestamp, timestamp)\n",
        "\n",
        "                        total_posts_fetched += 1\n",
        "\n",
        "                    # Check if we've fetched enough posts\n",
        "                    if total_posts_fetched >= 1000:\n",
        "                        break\n",
        "\n",
        "                    # Update after_timestamp to get older posts in the next iteration\n",
        "                    after_timestamp = posts[-1].created_utc if posts else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Save the last fetched timestamp to a file\n",
        "with open(last_timestamp_file, 'w') as f:\n",
        "    f.write(str(last_fetched_timestamp))\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTpTbsr7qsvA",
        "outputId": "53d9bc66-a3e7-4a3b-eecc-2d8dd1d85dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Searching for topic: Is this a scam?\n",
            "Skipping post 1fow3eu (already fetched)\n",
            "Skipping post 1fovnon (already fetched)\n",
            "Skipping post 1fovcm2 (already fetched)\n",
            "Skipping post 1fou993 (already fetched)\n",
            "Skipping post 1fott3c (already fetched)\n",
            "Skipping post 1fotohk (already fetched)\n",
            "Skipping post 1fot6kx (already fetched)\n",
            "Skipping post 1foseuh (already fetched)\n",
            "Skipping post 1fosdal (already fetched)\n",
            "Skipping post 1forq8i (already fetched)\n",
            "Skipping post 1foqzrs (already fetched)\n",
            "Skipping post 1foqpwr (already fetched)\n",
            "Skipping post 1foq9r2 (already fetched)\n",
            "Skipping post 1fopppb (already fetched)\n",
            "Skipping post 1fopdi4 (already fetched)\n",
            "Skipping post 1fop8e5 (already fetched)\n",
            "Skipping post 1fop1fk (already fetched)\n",
            "Skipping post 1fongba (already fetched)\n",
            "Skipping post 1fong93 (already fetched)\n",
            "Skipping post 1fomvs0 (already fetched)\n",
            "Stopping search at post 1fom0n7 (older than last fetched)\n",
            "Error during post retrieval: 'ListingGenerator' object is not subscriptable\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ro0l70pLrZob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bp1zfk0Qt85D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PushShift"
      ],
      "metadata": {
        "id": "J4wrDp_Yt9Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y06Cz5uxuUP4",
        "outputId": "f614d77f-4803-43cc-f05f-692db9c697e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmaw in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pmaw) (2.32.3)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (from pmaw) (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw->pmaw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw->pmaw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw->pmaw) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pmaw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pmaw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pmaw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pmaw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
      ],
      "metadata": {
        "id": "h_ng3arlCqP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "from pmaw import PushshiftAPI\n",
        "import time\n",
        "\n",
        "# Initialize the Pushshift API client\n",
        "api = PushshiftAPI()\n",
        "\n",
        "# Specify the subreddit\n",
        "subreddit_name = 'Scams'\n",
        "\n",
        "# Create directories to save images and videos\n",
        "os.makedirs('media', exist_ok=True)\n",
        "\n",
        "# Define the CSV and zip file paths\n",
        "csv_file = 'reddit_posts_data.csv'\n",
        "zip_file = 'media_files.zip'\n",
        "\n",
        "# File to store previously fetched post IDs\n",
        "post_id_log = 'fetched_posts.txt'\n",
        "# File to store the last fetched timestamp\n",
        "last_timestamp_file = 'last_fetched_timestamp.txt'\n",
        "\n",
        "# Function to download media\n",
        "def download_media(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Load previously fetched post IDs from the log file\n",
        "if os.path.exists(post_id_log):\n",
        "    with open(post_id_log, 'r') as f:\n",
        "        fetched_post_ids = set(f.strip() for f in f.readlines())\n",
        "else:\n",
        "    fetched_post_ids = set()\n",
        "\n",
        "# Load the last fetched timestamp\n",
        "if os.path.exists(last_timestamp_file):\n",
        "    with open(last_timestamp_file, 'r') as f:\n",
        "        last_fetched_timestamp = float(f.read().strip())\n",
        "else:\n",
        "    last_fetched_timestamp = 0  # Start from the beginning if the file does not exist\n",
        "\n",
        "# Open CSV file for appending (to avoid overwriting previous data)\n",
        "with open(csv_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Post ID', 'Title', 'Author', 'Score', 'Post URL', 'Text', 'Media Filename', 'Has Media', 'Topic', 'Demographics Region', 'Timestamp', 'Date', 'Month', 'Year']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # If the CSV file is new, write the header row\n",
        "    if os.stat(csv_file).st_size == 0:\n",
        "        writer.writeheader()\n",
        "\n",
        "    print(\"Fetching posts...\")\n",
        "\n",
        "    # Search for posts containing the topics \"Is this a scam?\" or \"Victim of a scam\"\n",
        "    topics = [\"Is this a scam?\", \"Victim of a scam\"]\n",
        "    total_posts_fetched = 0\n",
        "    limit = 10000  # Set the total number of posts to fetch\n",
        "    batch_size = 500  # Number of posts per batch\n",
        "\n",
        "    for topic in topics:\n",
        "        print(f\"Searching for topic: {topic}\")\n",
        "        try:\n",
        "            # Pagination support for fetching more posts\n",
        "            last_timestamp_in_loop = None\n",
        "            while total_posts_fetched < limit:\n",
        "                # Fetch posts newer than the last timestamp\n",
        "                if last_timestamp_in_loop:\n",
        "                    posts = api.search_submissions(subreddit=subreddit_name, q=topic, before=last_timestamp_in_loop, limit=batch_size)\n",
        "                else:\n",
        "                    posts = api.search_submissions(subreddit=subreddit_name, q=topic, limit=batch_size)\n",
        "\n",
        "                if not posts:\n",
        "                    print(\"No more posts found.\")\n",
        "                    break\n",
        "\n",
        "                for post in posts:\n",
        "                    post_id = post['id']\n",
        "\n",
        "                    # Skip posts already fetched\n",
        "                    if post_id in fetched_post_ids:\n",
        "                        print(f\"Skipping post {post_id} (already fetched)\")\n",
        "                        continue\n",
        "\n",
        "                    # Only consider posts newer than the last fetched timestamp\n",
        "                    if post['created_utc'] <= last_fetched_timestamp:\n",
        "                        print(f\"Skipping post {post_id} (not newer than last fetched)\")\n",
        "                        continue\n",
        "\n",
        "                    # Extract relevant information from the post\n",
        "                    title = post['title']\n",
        "                    author = post.get('author', 'Unknown')\n",
        "                    score = post['score']\n",
        "                    post_url = post.get('url', '')\n",
        "                    text = post.get('selftext', '')\n",
        "                    media_filename = ''\n",
        "                    has_media = 'No'\n",
        "                    timestamp = post['created_utc']\n",
        "                    post_date = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "                    post_month = datetime.utcfromtimestamp(timestamp).strftime('%B')\n",
        "                    post_year = datetime.utcfromtimestamp(timestamp).strftime('%Y')\n",
        "\n",
        "                    # Check if the post has an image or video\n",
        "                    if post_url.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_image{os.path.splitext(post_url)[1]}\")\n",
        "                        download_media(post_url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "                    elif post_url.endswith(('.mp4', '.webm')):\n",
        "                        media_filename = os.path.join('media', f\"{post_id}_video{os.path.splitext(post_url)[1]}\")\n",
        "                        download_media(post_url, media_filename)\n",
        "                        has_media = 'Yes'\n",
        "\n",
        "                    # Write post data to CSV\n",
        "                    writer.writerow({\n",
        "                        'Post ID': post_id,\n",
        "                        'Title': title,\n",
        "                        'Author': author,\n",
        "                        'Score': score,\n",
        "                        'Post URL': post_url,\n",
        "                        'Text': text,\n",
        "                        'Media Filename': media_filename,\n",
        "                        'Has Media': has_media,\n",
        "                        'Topic': topic,\n",
        "                        'Demographics Region': 'Unknown',  # Demographics can be added if applicable\n",
        "                        'Timestamp': timestamp,\n",
        "                        'Date': post_date,\n",
        "                        'Month': post_month,\n",
        "                        'Year': post_year\n",
        "                    })\n",
        "\n",
        "                    print(f\"Processed post ID: {post_id}\")\n",
        "                    total_posts_fetched += 1\n",
        "\n",
        "                    # Save the post ID to the log file\n",
        "                    with open(post_id_log, 'a') as id_log_file:\n",
        "                        id_log_file.write(f\"{post_id}\\n\")\n",
        "                    fetched_post_ids.add(post_id)\n",
        "\n",
        "                    # Update the last fetched timestamp\n",
        "                    last_timestamp_in_loop = timestamp\n",
        "\n",
        "                    # Break if we reach the limit\n",
        "                    if total_posts_fetched >= limit:\n",
        "                        break\n",
        "\n",
        "                # Sleep to avoid rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during post retrieval: {e}\")\n",
        "\n",
        "# Save the last fetched timestamp to a file\n",
        "with open(last_timestamp_file, 'w') as f:\n",
        "    f.write(str(last_timestamp_in_loop or last_fetched_timestamp))\n",
        "\n",
        "# Zip media files\n",
        "with zipfile.ZipFile(zip_file, 'w') as media_zip:\n",
        "    for foldername, subfolders, filenames in os.walk('media'):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            media_zip.write(file_path, os.path.basename(file_path))\n",
        "\n",
        "print(f\"Media files have been zipped into {zip_file}\")\n",
        "print(\"Done fetching posts and saving data.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXSuPT95uArg",
        "outputId": "e78ea57f-4223-4dae-89d3-b5271d7c0a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts...\n",
            "Searching for topic: Is this a scam?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No more posts found.\n",
            "Searching for topic: Victim of a scam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No more posts found.\n",
            "Media files have been zipped into media_files.zip\n",
            "Done fetching posts and saving data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "seBh0zDbvVHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/1st_491_reddit_posts_data.csv')\n",
        "df2=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/2nd_reddit_posts_data.csv')\n",
        "df3=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/3rd_reddit_posts_data.csv')\n",
        "df4=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/4th_reddit_posts_data.csv')\n",
        "df5=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/5th_reddit_posts_data.csv')\n",
        "df6=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/6th_reddit_posts_data.csv')\n",
        "df7=pd.read_csv('/content/drive/MyDrive/SCVI/Final Data/7th_reddit_posts_data.csv')"
      ],
      "metadata": {
        "id": "0tDFLPCev-vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([df1, df2, df3, df4, df5, df6, df7], ignore_index=True)"
      ],
      "metadata": {
        "id": "7jPArgTEwpXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.to_csv('reddit_posts_data.csv', index=False)"
      ],
      "metadata": {
        "id": "U67NWYxHofVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRfZ0Bzaw2xc",
        "outputId": "0e19d828-da16-4fbe-de12-c9670c7e948a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3113, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_count = merged_df.duplicated(subset=['Post ID']).sum()\n",
        "\n",
        "print(f\"Total duplicate posts: {duplicate_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NQ8i9hYxRI8",
        "outputId": "ac49757d-8dc6-4dd5-826b-a57613619d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total duplicate posts: 1904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate posts based on a specific column (e.g., 'Post ID')\n",
        "# Adjust the column name as needed to identify duplicates\n",
        "cleaned_df = merged_df.drop_duplicates(subset=['Post ID'], keep='first')  # Keep the first occurrence\n",
        "\n",
        "# Optionally reset the index after dropping duplicates\n",
        "cleaned_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the shape of the cleaned DataFrame\n",
        "print(f\"Shape of the cleaned DataFrame: {cleaned_df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N37cxu4jxUMv",
        "outputId": "5bb10299-a477-479b-dfd3-651b71b6cd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the cleaned DataFrame: (1209, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.to_csv('reddit_posts_data_1200.csv', index=False)\n"
      ],
      "metadata": {
        "id": "RHkIN14hx7k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HvtXy4I4cte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Push_Shift new"
      ],
      "metadata": {
        "id": "QFcjEM544dGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "def getPushshiftData(query, after, before, sub):\n",
        "    # Build the URL with the relevant query parameters\n",
        "    url = f'https://api.pushshift.io/reddit/search/submission/?title={str(query)}&size=1000&after={str(after)}&before={str(before)}&subreddit={str(sub)}'\n",
        "\n",
        "    # Print the URL to track\n",
        "    print(f\"Fetching data from: {url}\")\n",
        "\n",
        "    # Make the API request\n",
        "    r = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if r.status_code != 200:\n",
        "        print(f\"Error: Received status code {r.status_code}\")\n",
        "        return None\n",
        "\n",
        "    # Load the JSON response into a Python dictionary\n",
        "    data = json.loads(r.text)\n",
        "\n",
        "    # Print the full response to inspect it\n",
        "    print(\"Full response:\", data)\n",
        "\n",
        "    # Safely check for 'data' key in the response\n",
        "    if 'data' in data:\n",
        "        return data['data']\n",
        "    else:\n",
        "        print(f\"Error: 'data' key not found in response. Full response: {data}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Initialize an empty dictionary to store submission data\n",
        "subStats = {}\n",
        "\n",
        "# Function to extract relevant data points from each submission\n",
        "def collectSubData(subm):\n",
        "    # subData holds the data points extracted from a single submission\n",
        "    subData = list()\n",
        "\n",
        "    # Extract the title, URL, author, score, etc.\n",
        "    title = subm['title']\n",
        "    url = subm['url']\n",
        "\n",
        "    # Try-except to handle missing flair data\n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"  # Handle missing flairs\n",
        "\n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc'])  # Convert timestamp to readable format\n",
        "    numComms = subm['num_comments']\n",
        "    permalink = subm['permalink']\n",
        "\n",
        "    # Store the extracted data in a tuple and append to the list\n",
        "    subData.append((sub_id, title, url, author, score, created, numComms, permalink, flair))\n",
        "\n",
        "    # Add the submission data to the global subStats dictionary\n",
        "    subStats[sub_id] = subData\n",
        "\n",
        "# Example usage of the functions:\n",
        "after_date = \"1514764800\"  # Example 'after' timestamp (Unix epoch format)\n",
        "before_date = \"1517443200\"  # Example 'before' timestamp\n",
        "query_term = \"screenshot\"\n",
        "subreddit = \"PS4\"\n",
        "\n",
        "# Fetch data using the constructed Pushshift URL\n",
        "data = getPushshiftData(query_term, after_date, before_date, subreddit)\n",
        "\n",
        "# Collect and store data points for each submission\n",
        "for submission in data:\n",
        "    collectSubData(submission)\n",
        "\n",
        "# Example output\n",
        "print(subStats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "zCUH7LXZ4g04",
        "outputId": "7ef9ed5b-2dd3-4e03-a71e-261d976a0fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from: https://api.pushshift.io/reddit/search/submission/?title=screenshot&size=1000&after=1514764800&before=1517443200&subreddit=PS4\n",
            "Error: Received status code 403\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6c752b2d39fa>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Collect and store data points for each submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mcollectSubData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def getPushshiftData(query, after, before, sub):\n",
        "    # Build the URL\n",
        "    url = f'https://api.pushshift.io/reddit/search/submission/?title={query}&size=1000&after={after}&before={before}&subreddit={sub}'\n",
        "\n",
        "    # Print the URL to track\n",
        "    print(f\"Fetching data from: {url}\")\n",
        "\n",
        "    # Add a User-Agent header\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; RedditDataBot/1.0; +https://www.yourwebsite.com)'}\n",
        "\n",
        "    # Make the API request with the headers\n",
        "    r = requests.get(url, headers=headers)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if r.status_code != 200:\n",
        "        print(f\"Error: Received status code {r.status_code}\")\n",
        "        return None\n",
        "\n",
        "    # Load the JSON response into a Python dictionary\n",
        "    data = json.loads(r.text)\n",
        "\n",
        "    # Safely check for 'data' key in the response\n",
        "    if 'data' in data:\n",
        "        return data['data']\n",
        "    else:\n",
        "        print(f\"Error: 'data' key not found in response. Full response: {data}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "query_term = \"scam\"\n",
        "after_date = \"1514764800\"  # Example Unix timestamp\n",
        "before_date = \"1517443200\"  # Example Unix timestamp\n",
        "subreddit = \"Scams\"\n",
        "\n",
        "data = getPushshiftData(query_term, after_date, before_date, subreddit)\n",
        "\n",
        "if data:\n",
        "    for submission in data:\n",
        "        collectSubData(submission)\n",
        "else:\n",
        "    print(\"No data received.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-o2VLfV5VQX",
        "outputId": "3a2a68d6-6efc-47b9-aaeb-96dd6b5401db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from: https://api.pushshift.io/reddit/search/submission/?title=scam&size=1000&after=1514764800&before=1517443200&subreddit=Scams\n",
            "Error: Received status code 403\n",
            "No data received.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n"
      ],
      "metadata": {
        "id": "AKE1YD7D4wtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "0ShaWJPh47f6",
        "outputId": "f5548dcb-7779-4f6c-b09e-113ce092dbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c5d84736ba45>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zstandard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PogaL5quJQSM",
        "outputId": "d2a955e7-95c6-4924-a509-f866df189668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zstandard\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard\n",
            "Successfully installed zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade zstandard\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad6cNPDPQSi9",
        "outputId": "a8b4b7f9-de1c-4274-ad34-1d4a8b4e0c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zstandard\n",
            "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard\n",
            "Successfully installed zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zstandard\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import logging.handlers\n",
        "\n",
        "# Default paths and fields\n",
        "input_file_path = \"/content/drive/MyDrive/SCVI/Historical/subreddits23/Scams_submissions.zst\"\n",
        "output_file_path = \"Scams_submissions.csv\"\n",
        "fields = []\n",
        "\n",
        "log = logging.getLogger(\"bot\")\n",
        "log.setLevel(logging.DEBUG)\n",
        "log.addHandler(logging.StreamHandler())\n",
        "\n",
        "\n",
        "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
        "    chunk = reader.read(chunk_size)\n",
        "    bytes_read += chunk_size\n",
        "    if previous_chunk is not None:\n",
        "        chunk = previous_chunk + chunk\n",
        "    try:\n",
        "        return chunk.decode()\n",
        "    except UnicodeDecodeError:\n",
        "        if bytes_read > max_window_size:\n",
        "            raise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
        "        return read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
        "\n",
        "\n",
        "def read_lines_zst(file_name):\n",
        "    with open(file_name, 'rb') as file_handle:\n",
        "        buffer = ''\n",
        "        reader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
        "        while True:\n",
        "            chunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
        "            if not chunk:\n",
        "                break\n",
        "            lines = (buffer + chunk).split(\"\\n\")\n",
        "\n",
        "            for line in lines[:-1]:\n",
        "                yield line, file_handle.tell()\n",
        "\n",
        "            buffer = lines[-1]\n",
        "        reader.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Filter out non-relevant arguments like '-f' from Jupyter or other environments\n",
        "    args = [arg for arg in sys.argv if not arg.startswith(\"-\")]\n",
        "\n",
        "    # Override defaults with command-line arguments if they exist\n",
        "    if len(args) >= 2:\n",
        "        input_file_path = args[1]\n",
        "        output_file_path = args[2] if len(args) >= 3 else output_file_path\n",
        "        fields = args[3].split(\",\") if len(args) >= 4 else fields\n",
        "\n",
        "    # Ensure the input file exists before proceeding\n",
        "    if not os.path.exists(input_file_path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
        "\n",
        "    # Determine if this is submission data\n",
        "    is_submission = \"submission\" in input_file_path\n",
        "    if not fields:\n",
        "        # Use default fields if none are provided\n",
        "        if is_submission:\n",
        "            fields = [\"author\", \"title\", \"score\", \"created\", \"link\", \"text\", \"url\"]\n",
        "        else:\n",
        "            fields = [\"author\", \"score\", \"created\", \"link\", \"body\"]\n",
        "\n",
        "    file_size = os.stat(input_file_path).st_size\n",
        "    file_lines, bad_lines = 0, 0\n",
        "    line, created = None, None\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding='utf-8', newline=\"\") as output_file:\n",
        "        writer = csv.writer(output_file)\n",
        "        writer.writerow(fields)\n",
        "        try:\n",
        "            for line, file_bytes_processed in read_lines_zst(input_file_path):\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    output_obj = []\n",
        "                    for field in fields:\n",
        "                        if field == \"created\":\n",
        "                            value = datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d %H:%M\")\n",
        "                        elif field == \"link\":\n",
        "                            if 'permalink' in obj:\n",
        "                                value = f\"https://www.reddit.com{obj['permalink']}\"\n",
        "                            else:\n",
        "                                value = f\"https://www.reddit.com/r/{obj['subreddit']}/comments/{obj['link_id'][3:]}/_/{obj['id']}/\"\n",
        "                        elif field == \"author\":\n",
        "                            value = f\"u/{obj['author']}\"\n",
        "                        elif field == \"text\":\n",
        "                            value = obj.get('selftext', \"\")\n",
        "                        else:\n",
        "                            value = obj.get(field, \"\")\n",
        "\n",
        "                        output_obj.append(str(value).encode(\"utf-8\", errors='replace').decode())\n",
        "                    writer.writerow(output_obj)\n",
        "\n",
        "                    created = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
        "                except json.JSONDecodeError:\n",
        "                    bad_lines += 1\n",
        "                file_lines += 1\n",
        "                if file_lines % 100000 == 0:\n",
        "                    log.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {file_lines:,} : {bad_lines:,} : {(file_bytes_processed / file_size) * 100:.0f}%\")\n",
        "        except KeyError as err:\n",
        "            log.info(f\"Object has no key: {err}\")\n",
        "            log.info(line)\n",
        "        except Exception as err:\n",
        "            log.info(err)\n",
        "            log.info(line)\n",
        "\n",
        "    log.info(f\"Complete : {file_lines:,} : {bad_lines:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfQrvI0HI_B3",
        "outputId": "ac0c1759-e1b4-41fc-b718-ac936feed791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "zstd decompress error: Unknown frame descriptor\n",
            "zstd decompress error: Unknown frame descriptor\n",
            "INFO:bot:zstd decompress error: Unknown frame descriptor\n",
            "None\n",
            "None\n",
            "INFO:bot:None\n",
            "Complete : 0 : 0\n",
            "Complete : 0 : 0\n",
            "INFO:bot:Complete : 0 : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import torch\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fields you want to extract from the JSON\n",
        "fields = [\"name\", \"author\", \"link_id\", \"subreddit\", \"body\", \"crreated_utc\", \"score\", \"ups\", \"downs\", \"controversility\", \"edited\", \"parent_id\", \"archived\"]\n",
        "\n",
        "# Step 1: Open the JSON file and load each line separately\n",
        "json_data = []\n",
        "with open('/content/drive/MyDrive/SCVI/Historical/subreddits23/Scams_comments', 'r') as json_file:\n",
        "    for line in json_file:\n",
        "        json_data.append(json.loads(line))  # Parse each line as a JSON object\n",
        "\n",
        "# Step 2: Transfer data to GPU RAM\n",
        "# For demonstration, we convert JSON strings to a tensor-like structure\n",
        "data_on_gpu = torch.tensor([len(json.dumps(item)) for item in json_data], device=device)\n",
        "print(f\"Data loaded to GPU RAM. Total items: {len(data_on_gpu)}\")\n",
        "\n",
        "# Step 3: Open/create a CSV file for writing\n",
        "with open('/content/drive/MyDrive/comments_output_file.csv', 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the headers (the specific fields you want)\n",
        "    csv_writer.writerow(fields)\n",
        "\n",
        "    # Write the rows (looping over the JSON data and extracting only the specified fields)\n",
        "    for item in json_data:\n",
        "        row = [item.get(field, \"\") for field in fields]  # Extract the fields, use empty string if the field is missing\n",
        "        csv_writer.writerow(row)\n",
        "\n",
        "print(\"JSON to CSV conversion successful!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1GUhO4xMw2-",
        "outputId": "394120ba-07c9-4de2-fd19-d6ff1ee2c56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Data loaded to GPU RAM. Total items: 2106655\n",
            "JSON to CSV conversion successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(raw_html):\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    return soup.get_text()"
      ],
      "metadata": {
        "id": "80ZEzz5JjQte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Load the CSV data into a DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/comments_output_file.csv')\n",
        "\n",
        "# Step 2: Define a function to clean HTML markup using BeautifulSoup\n",
        "def clean_html(raw_html):\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Step 3: Apply the HTML cleaning function to the 'self_text' or 'text' column\n",
        "df['clean_text'] = df['body'].apply(lambda x: clean_html(str(x)) if pd.notnull(x) else x)\n",
        "\n",
        "# Step 4: Display the cleaned DataFrame (Optional: Save it to a new CSV)\n",
        "print(df.head())\n",
        "\n",
        "# Step 5 (Optional): Save the cleaned DataFrame to a new CSV\n",
        "df.to_csv('/content/drive/MyDrive/cleaned_reddit_comments_data.csv', index=False)\n",
        "\n",
        "print(\"HTML markup removed and data saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irs9wg0Bkjrj",
        "outputId": "204cec1d-5921-4663-9999-327ed6eace30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b7f2fa32abde>:5: DtypeWarning: Columns (0,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/drive/MyDrive/comments_output_file.csv')\n",
            "<ipython-input-7-b7f2fa32abde>:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(raw_html, \"html.parser\")\n",
            "<ipython-input-7-b7f2fa32abde>:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(raw_html, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         name         author   link_id subreddit  \\\n",
            "0  t1_c08c730        Fishbum  t3_7uc5u     Scams   \n",
            "1  t1_c113dhm  PlatypusSpork  t3_dlk9g     Scams   \n",
            "2  t1_c2fhdld            db2  t3_jvn87     Scams   \n",
            "3  t1_c2fhdw0            db2  t3_jvn87     Scams   \n",
            "4  t1_c2fiy6d      truthdude  t3_jvn87     Scams   \n",
            "\n",
            "                                                body  crreated_utc  score  \\\n",
            "0         Why is this under Reports 1 and in yellow?           NaN      1   \n",
            "1  Scam. You will cash the check and they'll ask ...           NaN      2   \n",
            "2  Not a whole lot you can do, the return address...           NaN      0   \n",
            "3  Also, sell only locally or to someone who will...           NaN      2   \n",
            "4  Thank you. This seems to be sensible advice. :)\\n           NaN      1   \n",
            "\n",
            "   ups  downs  controversility edited   parent_id archived  \\\n",
            "0  1.0    0.0              NaN  False    t3_7uc5u     True   \n",
            "1  2.0    0.0              NaN  False    t3_dlk9g     True   \n",
            "2  0.0    0.0              NaN  False    t3_jvn87     True   \n",
            "3  2.0    0.0              NaN  False    t3_jvn87     True   \n",
            "4  1.0    0.0              NaN  False  t1_c2fhdw0     True   \n",
            "\n",
            "                                          clean_text  \n",
            "0         Why is this under Reports 1 and in yellow?  \n",
            "1  Scam. You will cash the check and they'll ask ...  \n",
            "2  Not a whole lot you can do, the return address...  \n",
            "3  Also, sell only locally or to someone who will...  \n",
            "4  Thank you. This seems to be sensible advice. :)\\n  \n",
            "HTML markup removed and data saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.head(1000)"
      ],
      "metadata": {
        "id": "zhrFMKLTmtAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"first_1000.csv\")"
      ],
      "metadata": {
        "id": "vQR5YKNSm0uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "input_file = '/content/drive/MyDrive/comments_output_file.csv'  # Your existing CSV file\n",
        "output_file = '/content/drive/MyDrive/comments_filtered_output_file.csv'  # Filtered CSV file\n",
        "\n",
        "# Step 1: Open the CSV file for reading\n",
        "with open(input_file, 'r', newline='', encoding='utf-8') as csv_in:\n",
        "    csv_reader = csv.DictReader(csv_in)  # Reading the CSV as a dictionary\n",
        "\n",
        "    # Step 2: Open/create a new CSV file for writing filtered data\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_out:\n",
        "        fieldnames = csv_reader.fieldnames  # Use the same field names as the input file\n",
        "        csv_writer = csv.DictWriter(csv_out, fieldnames=fieldnames)\n",
        "\n",
        "        # Write the headers to the new file\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        # Step 3: Filter the rows based on the 'text' field\n",
        "        for row in csv_reader:\n",
        "            text = row.get(\"selftext\", \"\").strip()  # Get and strip the 'text' field value\n",
        "\n",
        "            # Check if 'text' is not null, not '[removed]', and not '[deleted]'\n",
        "            if text and text.lower() not in ['[removed]', '[deleted]']:\n",
        "                csv_writer.writerow(row)  # Write the filtered row to the new CSV\n",
        "\n",
        "print(\"Filtered data successfully written to filtered_output_file.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdSsmoMNKQ3d",
        "outputId": "d21178a8-791e-4fce-92b4-37f92e8f0dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered data successfully written to filtered_output_file.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vyRdkKqAZDIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/SCVI/df_combined (1).csv')"
      ],
      "metadata": {
        "id": "rluo5LtlYpdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1704
        },
        "id": "9AprVm2bZMNH",
        "outputId": "47ccd06f-faa3-4683-ddd2-34bb4187e2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Post ID                                   Title                Author  \\\n",
              "0  1fovnon                 Potential discord scam?       Ok-Turnover9040   \n",
              "1  1fovcm2          pensionsai.co - is this legit?             Visit-Sea   \n",
              "2  1fou993     Metz media service, is this a scam?  Routine_University91   \n",
              "3  1fott3c              Swyftx text scam or not???  PermissionUnusual954   \n",
              "4  1fotohk  Can someone tell me if this is legit?         cheesecake3813   \n",
              "\n",
              "  Score                                           Post URL  \\\n",
              "0     1  https://www.reddit.com/r/Scams/comments/1fovno...   \n",
              "1     1  https://www.reddit.com/r/Scams/comments/1fovcm...   \n",
              "2     1  https://www.reddit.com/r/Scams/comments/1fou99...   \n",
              "3     2               https://i.redd.it/8bqej9117vqd1.jpeg   \n",
              "4     2                https://i.redd.it/59cn1ipt5vqd1.png   \n",
              "\n",
              "                                                Text  \\\n",
              "0  Usually it's pretty easy for me to spot a scam...   \n",
              "1  Does anybody have experience with www.pensions...   \n",
              "2  My dad’s friend asked my dad to register for t...   \n",
              "3  Hi guys,\\n\\nCan someone let me know if this is...   \n",
              "4  I know that I definitely fell for a scam but i...   \n",
              "\n",
              "             Media Filename Has Media            Topic Demographics Region  \\\n",
              "0                       NaN        No  Is this a scam?             Unknown   \n",
              "1                       NaN        No  Is this a scam?             Unknown   \n",
              "2                       NaN        No  Is this a scam?             Unknown   \n",
              "3  media/1fott3c_image.jpeg       Yes  Is this a scam?             Unknown   \n",
              "4   media/1fotohk_image.png       Yes  Is this a scam?             Unknown   \n",
              "\n",
              "   ... Unnamed: 54 Unnamed: 55 Unnamed: 56 Unnamed: 57 Unnamed: 58  \\\n",
              "0  ...         NaN         NaN         NaN         NaN         NaN   \n",
              "1  ...         NaN         NaN         NaN         NaN         NaN   \n",
              "2  ...         NaN         NaN         NaN         NaN         NaN   \n",
              "3  ...         NaN         NaN         NaN         NaN         NaN   \n",
              "4  ...         NaN         NaN         NaN         NaN         NaN   \n",
              "\n",
              "  Unnamed: 59 Unnamed: 60 Unnamed: 61 Unnamed: 62 Unnamed: 63  \n",
              "0         NaN         NaN         NaN         NaN         NaN  \n",
              "1         NaN         NaN         NaN         NaN         NaN  \n",
              "2         NaN         NaN         NaN         NaN         NaN  \n",
              "3         NaN         NaN         NaN         NaN         NaN  \n",
              "4         NaN         NaN         NaN         NaN         NaN  \n",
              "\n",
              "[5 rows x 64 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee0e7e6f-9596-450a-acd1-d80406b5acf4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Score</th>\n",
              "      <th>Post URL</th>\n",
              "      <th>Text</th>\n",
              "      <th>Media Filename</th>\n",
              "      <th>Has Media</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Demographics Region</th>\n",
              "      <th>...</th>\n",
              "      <th>Unnamed: 54</th>\n",
              "      <th>Unnamed: 55</th>\n",
              "      <th>Unnamed: 56</th>\n",
              "      <th>Unnamed: 57</th>\n",
              "      <th>Unnamed: 58</th>\n",
              "      <th>Unnamed: 59</th>\n",
              "      <th>Unnamed: 60</th>\n",
              "      <th>Unnamed: 61</th>\n",
              "      <th>Unnamed: 62</th>\n",
              "      <th>Unnamed: 63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1fovnon</td>\n",
              "      <td>Potential discord scam?</td>\n",
              "      <td>Ok-Turnover9040</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.reddit.com/r/Scams/comments/1fovno...</td>\n",
              "      <td>Usually it's pretty easy for me to spot a scam...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>Is this a scam?</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1fovcm2</td>\n",
              "      <td>pensionsai.co - is this legit?</td>\n",
              "      <td>Visit-Sea</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.reddit.com/r/Scams/comments/1fovcm...</td>\n",
              "      <td>Does anybody have experience with www.pensions...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>Is this a scam?</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1fou993</td>\n",
              "      <td>Metz media service, is this a scam?</td>\n",
              "      <td>Routine_University91</td>\n",
              "      <td>1</td>\n",
              "      <td>https://www.reddit.com/r/Scams/comments/1fou99...</td>\n",
              "      <td>My dad’s friend asked my dad to register for t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>No</td>\n",
              "      <td>Is this a scam?</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1fott3c</td>\n",
              "      <td>Swyftx text scam or not???</td>\n",
              "      <td>PermissionUnusual954</td>\n",
              "      <td>2</td>\n",
              "      <td>https://i.redd.it/8bqej9117vqd1.jpeg</td>\n",
              "      <td>Hi guys,\\n\\nCan someone let me know if this is...</td>\n",
              "      <td>media/1fott3c_image.jpeg</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Is this a scam?</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1fotohk</td>\n",
              "      <td>Can someone tell me if this is legit?</td>\n",
              "      <td>cheesecake3813</td>\n",
              "      <td>2</td>\n",
              "      <td>https://i.redd.it/59cn1ipt5vqd1.png</td>\n",
              "      <td>I know that I definitely fell for a scam but i...</td>\n",
              "      <td>media/1fotohk_image.png</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Is this a scam?</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 64 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee0e7e6f-9596-450a-acd1-d80406b5acf4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee0e7e6f-9596-450a-acd1-d80406b5acf4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee0e7e6f-9596-450a-acd1-d80406b5acf4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21d01a02-5c9b-408f-9e38-b641e148af9b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21d01a02-5c9b-408f-9e38-b641e148af9b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21d01a02-5c9b-408f-9e38-b641e148af9b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (64) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_scam_type_rows = df[df['Label'].isnull()]"
      ],
      "metadata": {
        "id": "kCz2H_TKZGbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_scam_type_rows.to_csv(\"dataset1.csv\")"
      ],
      "metadata": {
        "id": "dfSGrJLMZcjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this converts a zst file to csv\n",
        "#\n",
        "# it's important to note that the resulting file will likely be quite large\n",
        "# and you probably won't be able to open it in excel or another csv reader\n",
        "#\n",
        "# arguments are inputfile, outputfile, fields\n",
        "# call this like\n",
        "# python to_csv.py wallstreetbets_submissions.zst wallstreetbets_submissions.csv author,selftext,title\n",
        "\n",
        "import zstandard\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import logging.handlers\n",
        "\n",
        "\n",
        "# put the path to the input file\n",
        "input_file_path = r\"/content/drive/MyDrive/SCVI/Historical/subreddits23/Scams_submissions.zst\"\n",
        "# put the path to the output file, with the csv extension\n",
        "output_file_path = r\"submissions.csv\"\n",
        "# if you want a custom set of fields, put them in the following list. If you leave it empty the script will use a default set of fields\n",
        "fields = []\n",
        "\n",
        "log = logging.getLogger(\"bot\")\n",
        "log.setLevel(logging.DEBUG)\n",
        "log.addHandler(logging.StreamHandler())\n",
        "\n",
        "\n",
        "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
        "\tchunk = reader.read(chunk_size)\n",
        "\tbytes_read += chunk_size\n",
        "\tif previous_chunk is not None:\n",
        "\t\tchunk = previous_chunk + chunk\n",
        "\ttry:\n",
        "\t\treturn chunk.decode()\n",
        "\texcept UnicodeDecodeError:\n",
        "\t\tif bytes_read > max_window_size:\n",
        "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
        "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
        "\n",
        "\n",
        "def read_lines_zst(file_name):\n",
        "\twith open(file_name, 'rb') as file_handle:\n",
        "\t\tbuffer = ''\n",
        "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
        "\t\twhile True:\n",
        "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
        "\t\t\tif not chunk:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
        "\n",
        "\t\t\tfor line in lines[:-1]:\n",
        "\t\t\t\tyield line, file_handle.tell()\n",
        "\n",
        "\t\t\tbuffer = lines[-1]\n",
        "\t\treader.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tif len(sys.argv) >= 3:\n",
        "\t\tinput_file_path = sys.argv[1]\n",
        "\t\toutput_file_path = sys.argv[2]\n",
        "\t\tfields = sys.argv[3].split(\",\")\n",
        "\n",
        "\tis_submission = \"submission\" in input_file_path\n",
        "\tif not len(fields):\n",
        "\t\tif is_submission:\n",
        "\t\t\tfields = [\"author\",\"title\",\"score\",\"created\",\"link\",\"text\",\"url\"]\n",
        "\t\telse:\n",
        "\t\t\tfields = [\"author\",\"score\",\"created\",\"link\",\"body\"]\n",
        "\n",
        "\tfile_size = os.stat(input_file_path).st_size\n",
        "\tfile_lines, bad_lines = 0, 0\n",
        "\tline, created = None, None\n",
        "\toutput_file = open(output_file_path, \"w\", encoding='utf-8', newline=\"\")\n",
        "\twriter = csv.writer(output_file)\n",
        "\twriter.writerow(fields)\n",
        "\ttry:\n",
        "\t\tfor line, file_bytes_processed in read_lines_zst(input_file_path):\n",
        "\t\t\ttry:\n",
        "\t\t\t\tobj = json.loads(line)\n",
        "\t\t\t\toutput_obj = []\n",
        "\t\t\t\tfor field in fields:\n",
        "\t\t\t\t\tif field == \"created\":\n",
        "\t\t\t\t\t\tvalue = datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d %H:%M\")\n",
        "\t\t\t\t\telif field == \"link\":\n",
        "\t\t\t\t\t\tif 'permalink' in obj:\n",
        "\t\t\t\t\t\t\tvalue = f\"https://www.reddit.com{obj['permalink']}\"\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tvalue = f\"https://www.reddit.com/r/{obj['subreddit']}/comments/{obj['link_id'][3:]}/_/{obj['id']}/\"\n",
        "\t\t\t\t\telif field == \"author\":\n",
        "\t\t\t\t\t\tvalue = f\"u/{obj['author']}\"\n",
        "\t\t\t\t\telif field == \"text\":\n",
        "\t\t\t\t\t\tif 'selftext' in obj:\n",
        "\t\t\t\t\t\t\tvalue = obj['selftext']\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tvalue = \"\"\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tvalue = obj[field]\n",
        "\n",
        "\t\t\t\t\toutput_obj.append(str(value).encode(\"utf-8\", errors='replace').decode())\n",
        "\t\t\t\twriter.writerow(output_obj)\n",
        "\n",
        "\t\t\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
        "\t\t\texcept json.JSONDecodeError as err:\n",
        "\t\t\t\tbad_lines += 1\n",
        "\t\t\tfile_lines += 1\n",
        "\t\t\tif file_lines % 100000 == 0:\n",
        "\t\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {file_lines:,} : {bad_lines:,} : {(file_bytes_processed / file_size) * 100:.0f}%\")\n",
        "\texcept KeyError as err:\n",
        "\t\tlog.info(f\"Object has no key: {err}\")\n",
        "\t\tlog.info(line)\n",
        "\texcept Exception as err:\n",
        "\t\tlog.info(err)\n",
        "\t\tlog.info(line)\n",
        "\n",
        "\toutput_file.close()\n",
        "\tlog.info(f\"Complete : {file_lines:,} : {bad_lines:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RfNGLaJuM3Dv",
        "outputId": "484a71e1-31f1-464f-a2a1-da51ab985fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-82dc73336a72>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0moutput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mis_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"submission\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import zstandard as zstd\n",
        "from datetime import datetime\n",
        "\n",
        "def read_lines_zst(file_path, chunk_size=16384):\n",
        "    \"\"\"Generator to yield lines from a zst compressed file\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        dctx = zstd.ZstdDecompressor()\n",
        "        with dctx.stream_reader(f) as reader:\n",
        "            while True:\n",
        "                chunk = reader.read(chunk_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "                for line in chunk.splitlines():\n",
        "                    yield line.decode('utf-8'), reader.tell()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Filter out non-relevant arguments like '-f' from Jupyter or other environments\n",
        "    args = [arg for arg in sys.argv if not arg.startswith(\"-\")]\n",
        "\n",
        "    # Define default output path and fields\n",
        "    output_file_path = \"output.csv\"\n",
        "    fields = []\n",
        "\n",
        "    # Override defaults with command-line arguments if they exist\n",
        "    if len(args) >= 2:\n",
        "        input_file_path = args[1]\n",
        "        output_file_path = args[2] if len(args) >= 3 else output_file_path\n",
        "        fields = args[3].split(\",\") if len(args) >= 4 else fields\n",
        "\n",
        "    # Ensure the input file exists before proceeding\n",
        "    if not os.path.exists(input_file_path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
        "\n",
        "    # Determine if this is submission data\n",
        "    is_submission = \"submission\" in input_file_path\n",
        "    if not fields:\n",
        "        # Use default fields if none are provided\n",
        "        if is_submission:\n",
        "            fields = [\"author\", \"title\", \"score\", \"created\", \"link\", \"text\", \"url\"]\n",
        "        else:\n",
        "            fields = [\"author\", \"score\", \"created\", \"link\", \"body\"]\n",
        "\n",
        "    file_size = os.stat(input_file_path).st_size\n",
        "    file_lines, bad_lines = 0, 0\n",
        "    line, created = None, None\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding='utf-8', newline=\"\") as output_file:\n",
        "        writer = csv.writer(output_file)\n",
        "        writer.writerow(fields)  # Write the header row\n",
        "\n",
        "        try:\n",
        "            for line, file_bytes_processed in read_lines_zst(input_file_path):\n",
        "                try:\n",
        "                    obj = json.loads(line)  # Load the JSON object from each line\n",
        "                    output_obj = []\n",
        "                    for field in fields:\n",
        "                        if field == \"created\":\n",
        "                            # Convert Unix timestamp to readable date\n",
        "                            value = datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d %H:%M\")\n",
        "                        elif field == \"link\":\n",
        "                            # Construct Reddit link\n",
        "                            if 'permalink' in obj:\n",
        "                                value = f\"https://www.reddit.com{obj['permalink']}\"\n",
        "                            else:\n",
        "                                value = f\"https://www.reddit.com/r/{obj['subreddit']}/comments/{obj['link_id'][3:]}/_/{obj['id']}/\"\n",
        "                        elif field == \"author\":\n",
        "                            # Format author\n",
        "                            value = f\"u/{obj['author']}\"\n",
        "                        elif field == \"text\":\n",
        "                            # Extract post body\n",
        "                            value = obj.get('selftext', \"\")\n",
        "                        else:\n",
        "                            # For any other field, just get its value or an empty string if not present\n",
        "                            value = obj.get(field, \"\")\n",
        "\n",
        "                        # Append the value, handling any encoding issues\n",
        "                        output_obj.append(str(value).encode(\"utf-8\", errors='replace').decode())\n",
        "\n",
        "                    writer.writerow(output_obj)  # Write the row to the CSV\n",
        "\n",
        "                    created = datetime.utcfromtimestamp(int(obj['created_utc']))  # Track creation time\n",
        "                except json.JSONDecodeError:\n",
        "                    bad_lines += 1  # Track bad JSON lines\n",
        "                file_lines += 1\n",
        "\n",
        "                # Log progress every 100,000 lines\n",
        "                if file_lines % 100000 == 0:\n",
        "                    print(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {file_lines:,} lines processed, {bad_lines:,} bad lines.\")\n",
        "\n",
        "        except KeyError as err:\n",
        "            print(f\"Object missing key: {err}\")\n",
        "            print(f\"Offending line: {line}\")\n",
        "        except Exception as err:\n",
        "            print(f\"Error occurred: {err}\")\n",
        "            print(f\"Offending line: {line}\")\n",
        "\n",
        "    print(f\"Processing complete: {file_lines:,} lines processed, {bad_lines:,} bad lines.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGfdfdjtOLfx",
        "outputId": "3b3f5aec-6069-4f94-e9f4-837d31efe74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: zstd decompress error: Unknown frame descriptor\n",
            "Offending line: None\n",
            "Processing complete: 0 lines processed, 0 bad lines.\n"
          ]
        }
      ]
    }
  ]
}